---
title: "Predictive model classification in the context of an unbalanced sample"
author: "Thibault FUCHEZ"
date: "27/04/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package

```{r}
library (tidyverse) # collection of packages for data analysis
library(ISLR) # Data for an Introduction to Statistical Learning with Applications in R/ contain the "Caravan / insurance data" 
library(caret) # Classification And REgression Training
library(pROC) # Tools for visualizing, smoothing and comparing ROC.
```

# Applied Predictive modeling - Mx Kuhn, ... (16.9 Computing)

```{r}
data <- Caravan
summary(data)
str(data)
plot(data$Purchase)
```

There are several factor variables in the data set. Many of the factor levels
have nonstandard characters, such as “%,” commas, and other values. When
these are converted to dummy variable columns, the values violate the rules
for naming new variables. To bypass this issue, we re-encode the names to be
more simplistic:

```{r}
# recodeLevels <- function(x)
#   {
#   x <- as.numeric(x)
#   ## Add zeros to the text version:
#   x <- gsub(" ", "0",format(as.numeric(x)))
#   factor(x)
#   }
# 
# ## Find which columns are regular factors or ordered factors
# isOrdered <- unlist(lapply(data, is.ordered))
# isFactor <- unlist(lapply(data, is.factor))
# convertCols <- names(isOrdered)[isOrdered | isFactor]
# 
# for(i in convertCols) data[,i] <- recodeLevels(data[,i])
# 
# ## Make the level ' insurance ' the first factor level
# data$Purchase <- factor(as.character(data$Purchase),
#                         levels = rev(levels(data$Purchase)))
```

```{r}
for (i in col(data)) data[,i] <- as.factor(data[,i])
```

```{r}
str(data)
```
  
The training and test sets were created using stratiﬁed random sampling.

First, split the training set off:

```{r}
set.seed(156)
split1 <- createDataPartition(data$Purchase, p = .7)[[1]]
other <- data[-split1,]
training <- data[ split1,]
```

Now create the evaluation and test sets :

```{r}
set.seed(934)
split2 <- createDataPartition(other$Purchase, p = 1/3)[[1]]
evaluation <- other[ split2,]
testing <- other[-split2,]
```

Determine the predictor names :

```{r}
predictors <- names(training)[names(training) != "Purchase"]
```

Dummy variables are useful for several models being ﬁt in this section.
The randomForest function has a limitation that all factor predictors must not have more than 32 levels. (sure??)

The customer type predictor has 39 levels, so a predictor set of dummy variables is created for this and other models using the model.matrix function:

```{r}
## The first column is the intercept, which is eliminated:
trainingInd <- data.frame(model.matrix(Purchase ~ ., data = training))[,-1]
evaluationInd <- data.frame(model.matrix(Purchase ~ ., data = evaluation))[,-1]
testingInd <- data.frame(model.matrix(Purchase ~ ., data = testing))[,-1]
## Add the outcome back into the data set
trainingInd$Purchase <- training$Purchase
evaluationInd$Purchase <- evaluation$Purchase
testingInd$Purchase <- testing$Purchase
## Determine a predictor set without highly sparse and unbalanced distributions:
isNZV <- nearZeroVar(trainingInd)
noNZVSet <- names(trainingInd)[-isNZV]
```

To obtain perofrmance measures, two wrapper functions were created : 

```{r}
## For accuracy, Kappa, the area under the ROC curve,
## sensitivity and specificity:
fiveStats <- function(...) c(twoClassSummary(...),defaultSummary(...))
## Everything but the area under the ROC curve:
fourStats <- function (data, lev = levels(data$obs), model = NULL)
  {
accKapp <- postResample(data[, "pred"], data[, "obs"])
out <- c(accKapp,
sensitivity(data[, "pred"], data[, "obs"], lev[1]),
specificity(data[, "pred"], data[, "obs"], lev[2]))
names(out)[3:4] <- c("Sens", "Spec")
out
}
```

Two control functions are developed for situations when class probabilities
can be created and when they cannot:

```{r}
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE,
                     summaryFunction = fiveStats,
                     verboseIter = TRUE)
ctrlNoProb <- ctrl
ctrlNoProb$summaryFunction <- fourStats
ctrlNoProb$classProbs <- FALSE
```

The three baseline models were ﬁt with the syntax:

```{r}
set.seed(1410)
rfFit <- train(Purchase ~ ., data = trainingInd,
               method = "rf",
               trControl = ctrl,
               ntree = 1500,
               tuneLength = 5,
               metric = "ROC")

set.seed(1410)
lrFit <- train(Purchase ~ .,
               data = trainingInd[, noNZVSet],
               method = "glm",
               trControl = ctrl,
               metric = "ROC")

set.seed(1401)
fdaFit <- train(Purchase ~ ., data = training, 
                method = "fda",
                tuneGrid = data.frame(.degree = 1, .nprune = 1:25),
                metric = "ROC",
                trControl = ctrl)
```

A data frame is used to house the predictions from different models:

```{r}
evalResults <- data.frame(Purchase = evaluation$Purchase)
evalResults$RF <- predict(rfFit, newdata = evaluationInd,
                          type = "prob")[,1]
evalResults$FDA <- predict(fdaFit, newdata = evaluation[, predictors],
                           type = "prob")[,1]
evalResults$LogReg <- predict(lrFit, newdata = evaluationInd[, noNZVSet],
                              type = "prob")[,1]
```

The ROC and lift curves are created from these objects. For example:

```{r}
rfROC <- roc(evalResults$Purchase, evalResults$RF,
             levels = rev(levels(evalResults$Purchase)))
## Create labels for the models:
labs <- c(RF = "Random Forest", LogReg = "Logistic Regression",
        FDA = "FDA (MARS)")
lift1 <- lift(Purchase ~ RF + LogReg + FDA, data = evalResults, labels = labs)
```

```{r}
rfROC
```

```{r}
lift1
```

To plot the curves:

```{r}
plot(rfROC, legacy.axes = TRUE)
xyplot(lift1, ylab = "%Events Found", xlab = "%Customers Evaluated",
       lwd = 2, type = "l")
```

```{r}
str(trainingInd)
```

