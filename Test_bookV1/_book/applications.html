<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Applications | Models of classification in context of imbalanced datas</title>
  <meta name="description" content="First try of bookdown" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Applications | Models of classification in context of imbalanced datas" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="First try of bookdown" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Applications | Models of classification in context of imbalanced datas" />
  
  <meta name="twitter:description" content="First try of bookdown" />
  

<meta name="author" content="Thibault Fuchez" />


<meta name="date" content="2021-07-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="remedies.html"/>
<link rel="next" href="summary.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#document-format"><i class="fa fa-check"></i><b>1.2</b> Document format</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html"><i class="fa fa-check"></i><b>2</b> Metrics for classification tasks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#the-foundation-of-the-metrics-the-confusion-matrix"><i class="fa fa-check"></i><b>2.1</b> The foundation of the metrics: The Confusion Matrix</a></li>
<li class="chapter" data-level="2.2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#accuracy-and-error-rate"><i class="fa fa-check"></i><b>2.2</b> Accuracy and error rate</a></li>
<li class="chapter" data-level="2.3" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-positive-rate"><i class="fa fa-check"></i><b>2.3</b> True Positive rate</a></li>
<li class="chapter" data-level="2.4" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-negative-and-false-positive-rate"><i class="fa fa-check"></i><b>2.4</b> True Negative and False positive rate</a></li>
<li class="chapter" data-level="2.5" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#positive-prediction-value-precision"><i class="fa fa-check"></i><b>2.5</b> Positive prediction value : Precision</a></li>
<li class="chapter" data-level="2.6" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#f-measure"><i class="fa fa-check"></i><b>2.6</b> F-measure</a></li>
<li class="chapter" data-level="2.7" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#kappa"><i class="fa fa-check"></i><b>2.7</b> Kappa</a></li>
<li class="chapter" data-level="2.8" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#roc-curve"><i class="fa fa-check"></i><b>2.8</b> ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>3</b> Classifiers</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classifiers.html"><a href="classifiers.html#lda"><i class="fa fa-check"></i><b>3.1</b> LDA</a></li>
<li class="chapter" data-level="3.2" data-path="classifiers.html"><a href="classifiers.html#presentation"><i class="fa fa-check"></i><b>3.2</b> Presentation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classifiers.html"><a href="classifiers.html#learning-lda-model"><i class="fa fa-check"></i><b>3.2.1</b> Learning LDA model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classifiers.html"><a href="classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>3.3</b> logistic regression</a></li>
<li class="chapter" data-level="3.4" data-path="classifiers.html"><a href="classifiers.html#svm"><i class="fa fa-check"></i><b>3.4</b> SVM</a></li>
<li class="chapter" data-level="3.5" data-path="classifiers.html"><a href="classifiers.html#classification-tree"><i class="fa fa-check"></i><b>3.5</b> classification tree</a></li>
<li class="chapter" data-level="3.6" data-path="classifiers.html"><a href="classifiers.html#random-forest"><i class="fa fa-check"></i><b>3.6</b> random forest</a></li>
<li class="chapter" data-level="3.7" data-path="classifiers.html"><a href="classifiers.html#naives-bayes"><i class="fa fa-check"></i><b>3.7</b> Naives bayes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="remedies.html"><a href="remedies.html"><i class="fa fa-check"></i><b>4</b> Remedies</a>
<ul>
<li class="chapter" data-level="4.1" data-path="remedies.html"><a href="remedies.html#pre-processing-resampling"><i class="fa fa-check"></i><b>4.1</b> Pre processing resampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="remedies.html"><a href="remedies.html#random-up-and-down-sample"><i class="fa fa-check"></i><b>4.1.1</b> Random Up and Down sample :</a></li>
<li class="chapter" data-level="4.1.2" data-path="remedies.html"><a href="remedies.html#rose"><i class="fa fa-check"></i><b>4.1.2</b> ROSE</a></li>
<li class="chapter" data-level="4.1.3" data-path="remedies.html"><a href="remedies.html#smote-family-smotenc-blsmote-adasyn"><i class="fa fa-check"></i><b>4.1.3</b> Smote family (smoteNC, BLSMOTE, ADASYN)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="remedies.html"><a href="remedies.html#learning-method-tuning"><i class="fa fa-check"></i><b>4.2</b> Learning method tuning</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="remedies.html"><a href="remedies.html#metaparameters-tuning"><i class="fa fa-check"></i><b>4.2.1</b> Metaparameters tuning</a></li>
<li class="chapter" data-level="4.2.2" data-path="remedies.html"><a href="remedies.html#weighted-class"><i class="fa fa-check"></i><b>4.2.2</b> Weighted class</a></li>
<li class="chapter" data-level="4.2.3" data-path="remedies.html"><a href="remedies.html#direct-sensitive-learning-with-cart-classifer"><i class="fa fa-check"></i><b>4.2.3</b> direct sensitive learning with cart classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="remedies.html"><a href="remedies.html#post-processing-threesholding"><i class="fa fa-check"></i><b>4.3</b> post-processing threesholding</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#first-models"><i class="fa fa-check"></i><b>5.2</b> First Models</a></li>
<li class="chapter" data-level="5.3" data-path="applications.html"><a href="applications.html#preprocessing-resampling-methods"><i class="fa fa-check"></i><b>5.3</b> Preprocessing: Resampling methods</a></li>
<li class="chapter" data-level="5.4" data-path="applications.html"><a href="applications.html#direct-cost-sensisitive-learnig"><i class="fa fa-check"></i><b>5.4</b> Direct cost sensisitive learnig</a></li>
<li class="chapter" data-level="5.5" data-path="applications.html"><a href="applications.html#post-processing-threesholding-1"><i class="fa fa-check"></i><b>5.5</b> Post processing Threesholding</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>6</b> Summary</a>
<ul>
<li class="chapter" data-level="6.1" data-path="summary.html"><a href="summary.html#different-strategies"><i class="fa fa-check"></i><b>6.1</b> different strategies :</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Models of classification in context of imbalanced datas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="applications" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Applications</h1>
<div id="introduction-1" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>In order to illustrate and discuss the different remedies proposed in the previous chapter, we are handling each on different dataset. Hence we can make comparisons and try to measure their efficiency.</p>
<ul>
<li>Few words about classifiers:</li>
</ul>
<p>Our first choice as classifiers was to use LDA, LR, RF and SVM. having ascertained that LDA et LR give very similar results, we decide to substitute LR by naives bayes’classifier in order to proposed a richer experience. Notice that we first try to use glmnet instead of glm but it doesn’t deliver better results (see spot.rmd). It is not unexpected that LR and LDA give nearly predictions, indeed they both are linear models and litteracy confirms they both give quite similar results.</p>
<ul>
<li>Few words About the code :</li>
</ul>
<p>We don’t introduce here all the manipulations done on the datasets, either the preparation of the dataset. You can find them in this <a href="https://github.com/arkansoap/Memoire-M1">github repositery</a> wich contains the .rmd for each dataset. In this repositery, you can also find the .R file which contains also the functions we code in order to avoid to many repetition in the code. At last, the alldat.Rdata stocked all objects built in the .rmd, it is used here to call the object we need. A fourth data set is in development that is not use here (creditcard).</p>
<p>Our first think was to put the tuning of parameters inherent to classifiers as a track of remedies. We eventually decided to launch our first models with the parameters already tune. First because there was no significant improvement in detection power, then because svm models was unable to give results with the basic parameters.</p>
<p>To illustrate some remdies introduced above, We choose three dataset with different level of imabalanced.</p>
<p>Let’s briefly presents those datasets:</p>
<ul>
<li>Spotify : a database including songs from spotify servers with categorical (about the qualification genre) and numerical predictors (about sound aspect of the songs computed by algorithm). Here the predicted variable is the popularity (is low /is not low).</li>
<li>Recidivism : a database containing arrested people and some caterogical predictors about their identity and judicial priors.</li>
<li>Hacked : a database concerning system and hacked tentative. a variable multiple_offense with “1” if hacked and “0” if not.</li>
</ul>
<p>The table 5.1 shows the priors of the depending class for the three datasets.</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-9">Table 5.1: </span>Table of priors ratio between positive and negative class
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
spot
</th>
<th style="text-align:left;">
hack
</th>
<th style="text-align:left;">
recid
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
0.807562076749436
</td>
<td style="text-align:left;">
0.954514596062458
</td>
<td style="text-align:left;">
0.88374113218474
</td>
</tr>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
0.192437923250564
</td>
<td style="text-align:left;">
0.0454854039375424
</td>
<td style="text-align:left;">
0.11625886781526
</td>
</tr>
</tbody>
</table>
</div>
<div id="first-models" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> First Models</h2>
<p>We start to compute classifiers algorithm .For now, we just want to observe results with basic parameters. This first computation can be used as a start reference to measure the remedies tested later.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-10">Table 5.2: </span>Confusion matrix spotify
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
0
</th>
<th style="text-align:left;">
1
</th>
<th style="text-align:left;">
Sum
</th>
<th style="text-align:left;">
0
</th>
<th style="text-align:left;">
1
</th>
<th style="text-align:left;">
Sum
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
rf
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
bayes
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
</tr>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
4431
</td>
<td style="text-align:left;">
975
</td>
<td style="text-align:left;">
5406
</td>
<td style="text-align:left;">
4373
</td>
<td style="text-align:left;">
983
</td>
<td style="text-align:left;">
5356
</td>
</tr>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
147
</td>
<td style="text-align:left;">
115
</td>
<td style="text-align:left;">
262
</td>
<td style="text-align:left;">
205
</td>
<td style="text-align:left;">
107
</td>
<td style="text-align:left;">
312
</td>
</tr>
<tr>
<td style="text-align:left;">
Sum
</td>
<td style="text-align:left;">
4578
</td>
<td style="text-align:left;">
1090
</td>
<td style="text-align:left;">
5668
</td>
<td style="text-align:left;">
4578
</td>
<td style="text-align:left;">
1090
</td>
<td style="text-align:left;">
5668
</td>
</tr>
<tr>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
lda
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
svm
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
<td style="text-align:left;background-color: lightgrey !important;">
</td>
</tr>
<tr>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
4559
</td>
<td style="text-align:left;">
1078
</td>
<td style="text-align:left;">
5637
</td>
<td style="text-align:left;">
4566
</td>
<td style="text-align:left;">
1075
</td>
<td style="text-align:left;">
5641
</td>
</tr>
<tr>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
19
</td>
<td style="text-align:left;">
12
</td>
<td style="text-align:left;">
31
</td>
<td style="text-align:left;">
12
</td>
<td style="text-align:left;">
15
</td>
<td style="text-align:left;">
27
</td>
</tr>
<tr>
<td style="text-align:left;">
Sum
</td>
<td style="text-align:left;">
4578
</td>
<td style="text-align:left;">
1090
</td>
<td style="text-align:left;">
5668
</td>
<td style="text-align:left;">
4578
</td>
<td style="text-align:left;">
1090
</td>
<td style="text-align:left;">
5668
</td>
</tr>
</tbody>
</table>
<p>The table 5.2 shows the confusion matrix resulting to the four classifiers used on the spotofy dataset. We observe the unabilty to properly predict the unpopular songs. A look on the metrics (table5.3) sharps this observation.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-11">Table 5.3: </span>Spotify metrics
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
accuracy
</th>
<th style="text-align:left;">
FNrate
</th>
<th style="text-align:left;">
TPrate
</th>
<th style="text-align:left;">
kappa
</th>
<th style="text-align:left;">
PrecisionPPV
</th>
<th style="text-align:left;">
Fscore
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rf
</td>
<td style="text-align:left;">
0.8020466
</td>
<td style="text-align:left;">
0.8944954
</td>
<td style="text-align:left;">
0.1055046
</td>
<td style="text-align:left;">
0.1032829
</td>
<td style="text-align:left;">
0.4389313
</td>
<td style="text-align:left;">
0.1701183
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayes
</td>
<td style="text-align:left;">
0.7904023
</td>
<td style="text-align:left;">
0.9018349
</td>
<td style="text-align:left;">
0.09816514
</td>
<td style="text-align:left;">
0.07332293
</td>
<td style="text-align:left;">
0.3429487
</td>
<td style="text-align:left;">
0.1526391
</td>
</tr>
<tr>
<td style="text-align:left;">
lda
</td>
<td style="text-align:left;">
0.8064573
</td>
<td style="text-align:left;">
0.9889908
</td>
<td style="text-align:left;">
0.01100917
</td>
<td style="text-align:left;">
0.01088917
</td>
<td style="text-align:left;">
0.3870968
</td>
<td style="text-align:left;">
0.02140946
</td>
</tr>
<tr>
<td style="text-align:left;">
svm
</td>
<td style="text-align:left;">
0.8082216
</td>
<td style="text-align:left;">
0.9862385
</td>
<td style="text-align:left;">
0.01376147
</td>
<td style="text-align:left;">
0.01772557
</td>
<td style="text-align:left;">
0.5555556
</td>
<td style="text-align:left;">
0.02685765
</td>
</tr>
</tbody>
</table>
<p>First we note that accuracy is very good, wich confirms accuracy is not a relaible metrics concerning imbalance dataset. A simple view on Detection power(TPr) shows that we don’t achieve to predict what songs are very unpopular. FN rate is obviously good because of the imblalanced ratio.</p>
<p>Let see the plot curve for all datasets.</p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-12-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Spotify and recid faces a real problem, no classifiers is able to make good predictions. Hacked data set has pretty good results for a first try.
It is possible that the nature of the data explains a part of this gap. Maybe variables can have some colinerarity or just don’t have a real dependency for the dependant variable.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-13">Table 5.4: </span>Hacked metrics
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
accuracy
</th>
<th style="text-align:left;">
FNrate
</th>
<th style="text-align:left;">
TPrate
</th>
<th style="text-align:left;">
kappa
</th>
<th style="text-align:left;">
PrecisionPPV
</th>
<th style="text-align:left;">
Fscore
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rf
</td>
<td style="text-align:left;">
0.9949815
</td>
<td style="text-align:left;">
0.05813953
</td>
<td style="text-align:left;">
0.9418605
</td>
<td style="text-align:left;">
0.9419781
</td>
<td style="text-align:left;">
0.9473684
</td>
<td style="text-align:left;">
0.9446064
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayes
</td>
<td style="text-align:left;">
0.9477021
</td>
<td style="text-align:left;">
0.872093
</td>
<td style="text-align:left;">
0.127907
</td>
<td style="text-align:left;">
0.1597342
</td>
<td style="text-align:left;">
0.3142857
</td>
<td style="text-align:left;">
0.1818182
</td>
</tr>
<tr>
<td style="text-align:left;">
lda
</td>
<td style="text-align:left;">
0.9537771
</td>
<td style="text-align:left;">
0.7732558
</td>
<td style="text-align:left;">
0.2267442
</td>
<td style="text-align:left;">
0.287576
</td>
<td style="text-align:left;">
0.4814815
</td>
<td style="text-align:left;">
0.3083004
</td>
</tr>
<tr>
<td style="text-align:left;">
svm
</td>
<td style="text-align:left;">
0.9635499
</td>
<td style="text-align:left;">
0.7034884
</td>
<td style="text-align:left;">
0.2965116
</td>
<td style="text-align:left;">
0.4098061
</td>
<td style="text-align:left;">
0.75
</td>
<td style="text-align:left;">
0.425
</td>
</tr>
</tbody>
</table>
<p>On table 5.4, we can see that only RF has very good results. Detection power and Fscore allows to judge the performance classifiers, contrary to accuracy or FN rate which are not usefull here. RF has the better results, In second position comes svm, even if he detects only a third among True positive, its quite a good results for a first try with an imbalanced data set, and Fscore ank kappa are not so bad.</p>
</div>
<div id="preprocessing-resampling-methods" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Preprocessing: Resampling methods</h2>
<p>As we can see with the function <em>resamp</em> (in Funclib.R of this<a href="https://github.com/arkansoap/Memoire-M1">github repositery</a> )the funclin.R and the .rmd, we try many resampling with different method. As the smote methods seems to us pertinent with this approach of oversampling with some logic, we try some development as adasyn or BLSMOTE. We made some test with random up sampling and downsizing which don’t stands here.</p>
<p>Note that the original smote algorithm is only for numerical variable. Some databases have few or no numerical variable so it is not an issue but a dataset as recid (with many caterogicla variable) asks to act on this variable. We could try to transform them as numerical but we choose to use SMoteNC, which is developed to deal with bot numerical and categorical variable.</p>
<p>Table 5.5 shows the results of SMOTE-NC resampling.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-14">Table 5.5: </span>Spot with Smote-NC
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
accuracy
</th>
<th style="text-align:left;">
FNrate
</th>
<th style="text-align:left;">
TPrate
</th>
<th style="text-align:left;">
kappa
</th>
<th style="text-align:left;">
PrecisionPPV
</th>
<th style="text-align:left;">
Fscore
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rf
</td>
<td style="text-align:left;">
0.8278724
</td>
<td style="text-align:left;">
0.2075142
</td>
<td style="text-align:left;">
0.7924858
</td>
<td style="text-align:left;">
0.6557449
</td>
<td style="text-align:left;">
0.8528444
</td>
<td style="text-align:left;">
0.821558
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayes
</td>
<td style="text-align:left;">
0.6241809
</td>
<td style="text-align:left;">
0.2647444
</td>
<td style="text-align:left;">
0.7352556
</td>
<td style="text-align:left;">
0.2483617
</td>
<td style="text-align:left;">
0.6016086
</td>
<td style="text-align:left;">
0.6617517
</td>
</tr>
<tr>
<td style="text-align:left;">
lda
</td>
<td style="text-align:left;">
0.6021188
</td>
<td style="text-align:left;">
0.3934032
</td>
<td style="text-align:left;">
0.6065968
</td>
<td style="text-align:left;">
0.2042377
</td>
<td style="text-align:left;">
0.6012124
</td>
<td style="text-align:left;">
0.6038926
</td>
</tr>
<tr>
<td style="text-align:left;">
svm
</td>
<td style="text-align:left;">
0.7035824
</td>
<td style="text-align:left;">
0.2505461
</td>
<td style="text-align:left;">
0.7494539
</td>
<td style="text-align:left;">
0.4071647
</td>
<td style="text-align:left;">
0.6864746
</td>
<td style="text-align:left;">
0.7165831
</td>
</tr>
</tbody>
</table>
<p>On graphics below, Lets compare RocCurve before and after smote resampling. This graphics illustrate a situation we faces a lot during our works. It often occurs with all smotefamily resamplers and Upsampling. We clearly see a global improvement. Random forest leads the course, following by SVM.</p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="direct-cost-sensisitive-learnig" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Direct cost sensisitive learnig</h2>
<p>Using a decision tree with a cost matrix give interesting results.
A question we ask is : what is the optimal cost to give at FP and FN rate?
Intuitively, we want to use the ratio of priors, for example, spotify priors are approximately 0,8 and 0,2 so we use c(1,0) = 1 and c(0,1) = 4, or with recid c(0,1) = 19 (0.95/0.005).</p>
<p>The graphics below shows evolution of sensitivity and specificity in function of the given cost.</p>
<p>The first one confirms that priors can be a good choice to choose the cost for confusion matrix. Globally, CART with cost has improve the predictions in comparison of first models. Not as good as resampling, but it gives substantial improvement. Kappa stills a bit low. A look on the left graph can help users to choose a cost slightly on the right to improove detection power if the cost in specificity is not too expensive for his application of the prediction.</p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Here, CART gives excellent result, as shows the kappa measure, still high which means results are not due to chance. Beside, TP and TN curves shows that a value between 50 and 90 seems optimal to maintain a high level of detection power and a low level of false alarm and stay balanced between them.</p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-18-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="post-processing-threesholding-1" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Post processing Threesholding</h2>
<p>In order to choose a good threeshold for the probabilities, we first vizualize the evolution of the three kind of errors. Those graphics will help us to choose a good threshold. We want that global error stays as high as possible. We also want a threeshold where FPrate and FNrate are closed to their intercept. At last we prefer situate on the right of the intercept because we privilegies FPrate to FNrate. the softer the slope is, the easier we can make gains on FP without looing too much in FN and accuracy. Hence, bayes and RF are better here for users who would try different threshold.</p>
<p><img src="Test_book_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Once we have choosen the better value depending our attempts, we can observe results in table 5.6 and reshape with another threshold if necessary.</p>
<table class="table table-striped table-hover" style="margin-left: auto; margin-right: auto;">
<caption>
<span id="tab:unnamed-chunk-20">Table 5.6: </span>metrics with new threshold
</caption>
<thead>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
accuracy
</th>
<th style="text-align:left;">
FNrate
</th>
<th style="text-align:left;">
TPrate
</th>
<th style="text-align:left;">
kappa
</th>
<th style="text-align:left;">
PrecisionPPV
</th>
<th style="text-align:left;">
Fscore
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
rf
</td>
<td style="text-align:left;">
0.6328511
</td>
<td style="text-align:left;">
0.3100917
</td>
<td style="text-align:left;">
0.6899083
</td>
<td style="text-align:left;">
0.2073542
</td>
<td style="text-align:left;">
0.3014028
</td>
<td style="text-align:left;">
0.4195258
</td>
</tr>
<tr>
<td style="text-align:left;">
Bayes
</td>
<td style="text-align:left;">
0.6264996
</td>
<td style="text-align:left;">
0.440367
</td>
<td style="text-align:left;">
0.559633
</td>
<td style="text-align:left;">
0.1438748
</td>
<td style="text-align:left;">
0.2714731
</td>
<td style="text-align:left;">
0.3655978
</td>
</tr>
<tr>
<td style="text-align:left;">
lda
</td>
<td style="text-align:left;">
0.6284404
</td>
<td style="text-align:left;">
0.4504587
</td>
<td style="text-align:left;">
0.5495413
</td>
<td style="text-align:left;">
0.1412709
</td>
<td style="text-align:left;">
0.270551
</td>
<td style="text-align:left;">
0.3625908
</td>
</tr>
<tr>
<td style="text-align:left;">
svm
</td>
<td style="text-align:left;">
0.7115385
</td>
<td style="text-align:left;">
0.6082569
</td>
<td style="text-align:left;">
0.3917431
</td>
<td style="text-align:left;">
0.1619352
</td>
<td style="text-align:left;">
0.305218
</td>
<td style="text-align:left;">
0.3431097
</td>
</tr>
</tbody>
</table>
<p>Once again, we observe a remrquable improvement in comparison of the first try. This threshold can be choosed by someone whants to keep accuracy above 60 percent and keep a balanced between specificity an sensitivity. If the user wants to win some detection power because he has still some margin in terms of specificity and accuracy, he can raise a bit the threshold.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="remedies.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="summary.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Test_book.pdf", "Test_book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
