<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Classifiers | Models of classification in context of imbalanced datas</title>
  <meta name="description" content="First try of bookdown" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Classifiers | Models of classification in context of imbalanced datas" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="First try of bookdown" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Classifiers | Models of classification in context of imbalanced datas" />
  
  <meta name="twitter:description" content="First try of bookdown" />
  

<meta name="author" content="Thibault Fuchez" />


<meta name="date" content="2021-07-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="metrics-for-classification-tasks.html"/>
<link rel="next" href="remedies.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#document-format"><i class="fa fa-check"></i><b>1.2</b> Document format</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html"><i class="fa fa-check"></i><b>2</b> Metrics for classification tasks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#the-foundation-of-the-metrics-the-confusion-matrix"><i class="fa fa-check"></i><b>2.1</b> The foundation of the metrics: The Confusion Matrix</a></li>
<li class="chapter" data-level="2.2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#accuracy-and-error-rate"><i class="fa fa-check"></i><b>2.2</b> Accuracy and error rate</a></li>
<li class="chapter" data-level="2.3" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-positive-rate"><i class="fa fa-check"></i><b>2.3</b> True Positive rate</a></li>
<li class="chapter" data-level="2.4" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-negative-and-false-positive-rate"><i class="fa fa-check"></i><b>2.4</b> True Negative and False positive rate</a></li>
<li class="chapter" data-level="2.5" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#positive-prediction-value-precision"><i class="fa fa-check"></i><b>2.5</b> Positive prediction value : Precision</a></li>
<li class="chapter" data-level="2.6" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#f-measure"><i class="fa fa-check"></i><b>2.6</b> F-measure</a></li>
<li class="chapter" data-level="2.7" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#kappa"><i class="fa fa-check"></i><b>2.7</b> Kappa</a></li>
<li class="chapter" data-level="2.8" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#roc-curve"><i class="fa fa-check"></i><b>2.8</b> ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>3</b> Classifiers</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classifiers.html"><a href="classifiers.html#lda"><i class="fa fa-check"></i><b>3.1</b> LDA</a></li>
<li class="chapter" data-level="3.2" data-path="classifiers.html"><a href="classifiers.html#presentation"><i class="fa fa-check"></i><b>3.2</b> Presentation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classifiers.html"><a href="classifiers.html#learning-lda-model"><i class="fa fa-check"></i><b>3.2.1</b> Learning LDA model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classifiers.html"><a href="classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>3.3</b> logistic regression</a></li>
<li class="chapter" data-level="3.4" data-path="classifiers.html"><a href="classifiers.html#svm"><i class="fa fa-check"></i><b>3.4</b> SVM</a></li>
<li class="chapter" data-level="3.5" data-path="classifiers.html"><a href="classifiers.html#classification-tree"><i class="fa fa-check"></i><b>3.5</b> classification tree</a></li>
<li class="chapter" data-level="3.6" data-path="classifiers.html"><a href="classifiers.html#random-forest"><i class="fa fa-check"></i><b>3.6</b> random forest</a></li>
<li class="chapter" data-level="3.7" data-path="classifiers.html"><a href="classifiers.html#naives-bayes"><i class="fa fa-check"></i><b>3.7</b> Naives bayes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="remedies.html"><a href="remedies.html"><i class="fa fa-check"></i><b>4</b> Remedies</a>
<ul>
<li class="chapter" data-level="4.1" data-path="remedies.html"><a href="remedies.html#pre-processing-resampling"><i class="fa fa-check"></i><b>4.1</b> Pre processing resampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="remedies.html"><a href="remedies.html#random-up-and-down-sample"><i class="fa fa-check"></i><b>4.1.1</b> Random Up and Down sample :</a></li>
<li class="chapter" data-level="4.1.2" data-path="remedies.html"><a href="remedies.html#rose"><i class="fa fa-check"></i><b>4.1.2</b> ROSE</a></li>
<li class="chapter" data-level="4.1.3" data-path="remedies.html"><a href="remedies.html#smote-family-smotenc-blsmote-adasyn"><i class="fa fa-check"></i><b>4.1.3</b> Smote family (smoteNC, BLSMOTE, ADASYN)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="remedies.html"><a href="remedies.html#learning-method-tuning"><i class="fa fa-check"></i><b>4.2</b> Learning method tuning</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="remedies.html"><a href="remedies.html#metaparameters-tuning"><i class="fa fa-check"></i><b>4.2.1</b> Metaparameters tuning</a></li>
<li class="chapter" data-level="4.2.2" data-path="remedies.html"><a href="remedies.html#weighted-class"><i class="fa fa-check"></i><b>4.2.2</b> Weighted class</a></li>
<li class="chapter" data-level="4.2.3" data-path="remedies.html"><a href="remedies.html#direct-sensitive-learning-with-cart-classifer"><i class="fa fa-check"></i><b>4.2.3</b> direct sensitive learning with cart classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="remedies.html"><a href="remedies.html#post-processing-threesholding"><i class="fa fa-check"></i><b>4.3</b> post-processing threesholding</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#first-models"><i class="fa fa-check"></i><b>5.2</b> First Models</a></li>
<li class="chapter" data-level="5.3" data-path="applications.html"><a href="applications.html#preprocessing-resampling-methods"><i class="fa fa-check"></i><b>5.3</b> Preprocessing: Resampling methods</a></li>
<li class="chapter" data-level="5.4" data-path="applications.html"><a href="applications.html#direct-cost-sensisitive-learnig"><i class="fa fa-check"></i><b>5.4</b> Direct cost sensisitive learnig</a></li>
<li class="chapter" data-level="5.5" data-path="applications.html"><a href="applications.html#post-processing-threesholding-1"><i class="fa fa-check"></i><b>5.5</b> Post processing Threesholding</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>6</b> Summary</a>
<ul>
<li class="chapter" data-level="6.1" data-path="summary.html"><a href="summary.html#different-strategies"><i class="fa fa-check"></i><b>6.1</b> different strategies :</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Models of classification in context of imbalanced datas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="classifiers" class="section level1" number="3">
<h1><span class="header-section-number">Chapter 3</span> Classifiers</h1>
<div id="lda" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> LDA</h2>
</div>
<div id="presentation" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Presentation</h2>
<p>The Linear Discriminant analysis classifier is used to predict the probability of belonging to a given class based on one or multiple predictor variables. It works with continuous and/or categorical predictor variables.</p>
<p>Linear discriminant analysis is a generalization of Fisher’s linear discriminant, a method used in statistics to find a linear combination of features that characterizes or separates two or more classes of objects.</p>
<p>In this approach, Fishers sought to ﬁnd the linear combination of the predictors such that the between-group variance was maximized relative to the within-group variance. In other words, he wanted to ﬁnd the combination of the predictors that gave maximum separation between the centers of the data while at the same time minimizing the variation within each group of data. <span class="citation">(<a href="#ref-lda" role="doc-biblioref">Li 2014</a>)</span></p>
<div id="learning-lda-model" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Learning LDA model</h3>
<p><span class="math inline">\(\mu_k\)</span> and <span class="math inline">\(\sigma_k\)</span> the mean and variance for the k-class.
LDA makes predictions by estimating the probability that a new set of inputs belongs to each class.
LDA makes some simplifiying assumptions about your data:
* that your data are gaussian
* that each distribute the same variance, so <span class="math inline">\(\sigma_0 = \sigma_1 = \sigma\)</span>
The model uses Bayes Theorem to estimate the probabilities.
<span class="math display">\[P(Y=c|X=x) = \frac{\pi_c f_c(x)}{\sum_{j=1}^k \pi_j f_j(x)}\]</span>
Where <span class="math inline">\(\pi_c\)</span> refers to the base probability of each class (c) observed in your training data. In Bayes’ Theorem this is called the prior probability.
<span class="math display">\[\pi_c=\frac{n_c}{n}\]</span>
Estimation of <span class="math inline">\(f_c\)</span> is more delicate, it is the estimated probability of <span class="math inline">\(x\)</span> belonging to the class. A Gaussian distribution function is used for <span class="math inline">\(f_c\)</span>. Plugging the Gaussian into the above equation and simplifying we end up with the equation below. This is called a discriminate function and the class is calculated as having the largest value will be the output classification :</p>
<p>x will be affected to the class c for which:
<span class="math display">\[\hat\delta_c(x) = \ln \hat\pi_c - \frac{\hat\mu_c^2}{2\hat\sigma^2} + x\frac{\hat\mu_c}{\hat\sigma^2}\]</span> is maximum.</p>
<p><span class="math inline">\(\hat\delta_c(x)\)</span> is the discriminate function for class c given input x, the <span class="math inline">\(mu_k\)</span>, <span class="math inline">\(sigma^2\)</span> and <span class="math inline">\(\pi_c\)</span> are all estimated from your data.</p>
</div>
</div>
<div id="logistic-regression" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> logistic regression</h2>
<p>In statistics, the logistic model (or logit model) is used to model the probability of a certain class. Each object being detected in the image would be assigned a probability between 0 and 1, with a sum of one. Logistic regression is a statistical model that in uses a logistic function to model a binary dependent variable. In regression analysis, logistic regression is estimating the parameters of a logistic model.</p>
<p>Binary Logistic Regression Major Assumptions:
* The dependent variable should be dichotomous in nature.
* There should be no outliers in the data, which can be assessed by converting the continuous predictors to standardized scores
* There should be no high correlations (multicollinearity) among the predictors. This can be assessed by a correlation matrix among the predictors. Tabachnick and Fidell suggest that as long correlation coefficients among independent variables are less than 0.90 the assumption is met <span class="citation">(<a href="#ref-log" role="doc-biblioref">Tabachnick and Fidell 2007</a>)</span>.</p>
<p>At the center of the logistic regression analysis is the task estimating the log odds of an event. Mathematically, logistic regression estimates a multiple linear regression function defined as:</p>
<p><span class="math display">\[logit(p)= \log \frac{p(y=1)}{1-p(y=1)}= \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_ix_i\]</span></p>
<p>Overfitting. When selecting the model for the logistic regression analysis, another important consideration is the model fit. Adding independent variables to a logistic regression model will always increase the amount of variance explained in the log odds (typically expressed as <span class="math inline">\(R^2\)</span>). However, adding more and more variables to the model can result in overfitting, which reduces the generalizability of the model beyond the data on which the model is fit.</p>
</div>
<div id="svm" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> SVM</h2>
<p>Support vector machine try to make a reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.</p>
<p>The support vector machines create an optimum hyperplane that separates the training data by the maximum margin. However, sometimes we would like to allow some misclassifications while separating categories. The SVM model has a cost function, which controls training errors and margins. For example, a small cost creates a large margin (a soft margin) and allows more misclassifications. On the other hand, a large cost creates a narrow margin (a hard margin) and permits fewer misclassifications. In this recipe, we will illustrate how the large and small cost will affect the SVM classifier.</p>
<p><img src="svm.PNG" style="display: block; margin: auto;" /></p>
</div>
<div id="classification-tree" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> classification tree</h2>
<p>Tree-based models consist of one or more nested if-then statements for the predictors that partition the data. Within these partitions, a model is used to predict the outcome.</p>
<p><em>Choosing the trees split points :</em></p>
<p>Technically, for regression modeling, the split cutoff is defined so that the residual sum of squared error (RSS) is minimized across the training samples that fall within the subpartition.</p>
<p>Recall that, the RSS is the sum of the squared difference between the observed outcome values and the predicted ones, <span class="math inline">\(RSS = \sum((Observeds - Predicteds)^2)\)</span>.</p>
<p>In classification settings, the split point is defined so that the population in subpartitions are pure as much as possible. Two measures of purity are generally used, including the Gini index and the entropy (or information gain).</p>
<p>For a given subpartition, <span class="math inline">\(Gini = \sum(p(1-p))\)</span> and <span class="math inline">\(entropy = -1\times \sum(p*log(p))\)</span>, where p is the proportion of misclassified observations within the subpartition.</p>
<p>The sum is computed across the different categories or classes in the outcome variable. The Gini index and the entropy varie from 0 (greatest purity) to 1 (maximum degree of impurity)</p>
<p><img src="tree.png" style="display: block; margin: auto;" /></p>
</div>
<div id="random-forest" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> random forest</h2>
<p>RF classifier is an ensemble method that trains several decision trees in parallel with bootstrapping followed by aggregation, jointly referred as bagging. Bootstrapping indicates that several individual decision trees are trained in parallel on various subsets of the training dataset using different subsets of available features. Bootstrapping ensures that each individual decision tree in the random forest is unique, which reduces the overall variance of the RF classifier. For the final decision, RF classifier aggregates the decisions of individual trees; consequently, RF classifier exhibits good generalization.</p>
</div>
<div id="naives-bayes" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Naives bayes</h2>
<p>basics of formula : <span class="math display">\[P(A ∩ B) = P(A) P(B|A) \iff P(B|A) = \frac{P(A ∩ B)}{P(A)}\]</span></p>
<p>In statistics, naive Bayes classifiers are a family of simple “probabilistic classifiers” based on applying Bayes’ theorem with strong (naïve) independence assumptions between the features</p>
<p>Naives : The joint probability calculation is simpler for independent events. so we consider events are independent. (it will be too complexe for more than two events)</p>
<p>Laplace correction will allow a small chance for these types of unforeseen circumstances (if joint event probabilty equals to 0.)</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-lda" class="csl-entry">
Li, Cheng. 2014. <em>Fisher Linear Discriminant Analysis</em>. Computer Science.
</div>
<div id="ref-log" class="csl-entry">
Tabachnick, Barbara G., and Linda S. Fidell. 2007. <em>Using Multivariate Statistics</em>. seventh edition.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="metrics-for-classification-tasks.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="remedies.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Test_book.pdf", "Test_book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
