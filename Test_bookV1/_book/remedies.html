<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Remedies | Models of classification in context of imbalanced datas</title>
  <meta name="description" content="First try of bookdown" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Remedies | Models of classification in context of imbalanced datas" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="First try of bookdown" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Remedies | Models of classification in context of imbalanced datas" />
  
  <meta name="twitter:description" content="First try of bookdown" />
  

<meta name="author" content="Thibault Fuchez" />


<meta name="date" content="2021-07-31" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classifiers.html"/>
<link rel="next" href="applications.html"/>
<script src="libs/header-attrs-2.8/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#document-format"><i class="fa fa-check"></i><b>1.2</b> Document format</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html"><i class="fa fa-check"></i><b>2</b> Metrics for classification tasks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#the-foundation-of-the-metrics-the-confusion-matrix"><i class="fa fa-check"></i><b>2.1</b> The foundation of the metrics: The Confusion Matrix</a></li>
<li class="chapter" data-level="2.2" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#accuracy-and-error-rate"><i class="fa fa-check"></i><b>2.2</b> Accuracy and error rate</a></li>
<li class="chapter" data-level="2.3" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-positive-rate"><i class="fa fa-check"></i><b>2.3</b> True Positive rate</a></li>
<li class="chapter" data-level="2.4" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#true-negative-and-false-positive-rate"><i class="fa fa-check"></i><b>2.4</b> True Negative and False positive rate</a></li>
<li class="chapter" data-level="2.5" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#positive-prediction-value-precision"><i class="fa fa-check"></i><b>2.5</b> Positive prediction value : Precision</a></li>
<li class="chapter" data-level="2.6" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#f-measure"><i class="fa fa-check"></i><b>2.6</b> F-measure</a></li>
<li class="chapter" data-level="2.7" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#kappa"><i class="fa fa-check"></i><b>2.7</b> Kappa</a></li>
<li class="chapter" data-level="2.8" data-path="metrics-for-classification-tasks.html"><a href="metrics-for-classification-tasks.html#roc-curve"><i class="fa fa-check"></i><b>2.8</b> ROC Curve</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="classifiers.html"><a href="classifiers.html"><i class="fa fa-check"></i><b>3</b> Classifiers</a>
<ul>
<li class="chapter" data-level="3.1" data-path="classifiers.html"><a href="classifiers.html#lda"><i class="fa fa-check"></i><b>3.1</b> LDA</a></li>
<li class="chapter" data-level="3.2" data-path="classifiers.html"><a href="classifiers.html#presentation"><i class="fa fa-check"></i><b>3.2</b> Presentation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="classifiers.html"><a href="classifiers.html#learning-lda-model"><i class="fa fa-check"></i><b>3.2.1</b> Learning LDA model</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="classifiers.html"><a href="classifiers.html#logistic-regression"><i class="fa fa-check"></i><b>3.3</b> logistic regression</a></li>
<li class="chapter" data-level="3.4" data-path="classifiers.html"><a href="classifiers.html#svm"><i class="fa fa-check"></i><b>3.4</b> SVM</a></li>
<li class="chapter" data-level="3.5" data-path="classifiers.html"><a href="classifiers.html#classification-tree"><i class="fa fa-check"></i><b>3.5</b> classification tree</a></li>
<li class="chapter" data-level="3.6" data-path="classifiers.html"><a href="classifiers.html#random-forest"><i class="fa fa-check"></i><b>3.6</b> random forest</a></li>
<li class="chapter" data-level="3.7" data-path="classifiers.html"><a href="classifiers.html#naives-bayes"><i class="fa fa-check"></i><b>3.7</b> Naives bayes</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="remedies.html"><a href="remedies.html"><i class="fa fa-check"></i><b>4</b> Remedies</a>
<ul>
<li class="chapter" data-level="4.1" data-path="remedies.html"><a href="remedies.html#pre-processing-resampling"><i class="fa fa-check"></i><b>4.1</b> Pre processing resampling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="remedies.html"><a href="remedies.html#random-up-and-down-sample"><i class="fa fa-check"></i><b>4.1.1</b> Random Up and Down sample :</a></li>
<li class="chapter" data-level="4.1.2" data-path="remedies.html"><a href="remedies.html#rose"><i class="fa fa-check"></i><b>4.1.2</b> ROSE</a></li>
<li class="chapter" data-level="4.1.3" data-path="remedies.html"><a href="remedies.html#smote-family-smotenc-blsmote-adasyn"><i class="fa fa-check"></i><b>4.1.3</b> Smote family (smoteNC, BLSMOTE, ADASYN)</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="remedies.html"><a href="remedies.html#learning-method-tuning"><i class="fa fa-check"></i><b>4.2</b> Learning method tuning</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="remedies.html"><a href="remedies.html#metaparameters-tuning"><i class="fa fa-check"></i><b>4.2.1</b> Metaparameters tuning</a></li>
<li class="chapter" data-level="4.2.2" data-path="remedies.html"><a href="remedies.html#weighted-class"><i class="fa fa-check"></i><b>4.2.2</b> Weighted class</a></li>
<li class="chapter" data-level="4.2.3" data-path="remedies.html"><a href="remedies.html#direct-sensitive-learning-with-cart-classifer"><i class="fa fa-check"></i><b>4.2.3</b> direct sensitive learning with cart classifer</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="remedies.html"><a href="remedies.html#post-processing-threesholding"><i class="fa fa-check"></i><b>4.3</b> post-processing threesholding</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="applications.html"><a href="applications.html"><i class="fa fa-check"></i><b>5</b> Applications</a>
<ul>
<li class="chapter" data-level="5.1" data-path="applications.html"><a href="applications.html#introduction-1"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="applications.html"><a href="applications.html#first-models"><i class="fa fa-check"></i><b>5.2</b> First Models</a></li>
<li class="chapter" data-level="5.3" data-path="applications.html"><a href="applications.html#preprocessing-resampling-methods"><i class="fa fa-check"></i><b>5.3</b> Preprocessing: Resampling methods</a></li>
<li class="chapter" data-level="5.4" data-path="applications.html"><a href="applications.html#direct-cost-sensisitive-learnig"><i class="fa fa-check"></i><b>5.4</b> Direct cost sensisitive learnig</a></li>
<li class="chapter" data-level="5.5" data-path="applications.html"><a href="applications.html#post-processing-threesholding-1"><i class="fa fa-check"></i><b>5.5</b> Post processing Threesholding</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>6</b> Summary</a>
<ul>
<li class="chapter" data-level="6.1" data-path="summary.html"><a href="summary.html#different-strategies"><i class="fa fa-check"></i><b>6.1</b> different strategies :</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Models of classification in context of imbalanced datas</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="remedies" class="section level1" number="4">
<h1><span class="header-section-number">Chapter 4</span> Remedies</h1>
<p>A lot of research have been made concerning this problem. Our goal is not to make an exhaustive review of all the technics to remedies this issue.
In this study, we will focus on methods than we can reproduce with our level of competence. It appears to us that it is interesting to separate the chosen methods in three levels :</p>
<ul>
<li>First, some remedies we can use before launching the machine learning algorithm (Pre-processing).</li>
<li>Secondly, some remedies we can use during the computation of a fitted models by the machine (learning method tuning).</li>
<li>At last, some remedies that can be used after the machine learning algorithm (post-processing).</li>
</ul>
<div id="pre-processing-resampling" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Pre processing resampling</h2>
<p>A lot of method exists in order to balanced a sample. Since the begin of the 21th century, a lot of studies appears in order to manage and discuss about sampling. Some methods are more sophisticated than other and might give better results, but they have no consensus on a method which have better results. Globally we can say it is the most common and popular remedies to counteract imbalanced data for two key reasons : we can use the sampled datas with all classifiers and it gives generally pretty goods results.</p>
<p>The first “basic” try of resampling is obviously try to make random over sampling or down-sizing. Over sampling consists in duplicate randomly positive cases in order to reach two class with approximately the same number of case. Down-sizing consists at randomly remove some negative cases in order to diminish the negative classes. Then we can easily imagine other way to select the cases removed or duplicate. Moreover, we can try to mix the two methods.</p>
<p>In her paper, Japkowicz discussed the effect of imbalance in a data set. Two resampling methods were considered. Random resampling consisted of resampling the smaller class at random until it consisted of as many samples as the majority class and “focused resampling” consisted of resampling only those minority examples that occurred on the boundary between the minority and majority classes. Random under-sampling was considered, which involved under-sampling the majority class samples at random until their numbers matched the number of minority class samples; focused under-sampling involved under-sampling the majority class samples lying further away. She noted that both the sampling approaches were effective, and she also observed that using the sophisticated sampling techniques did not give any clear advantage in the domain considered. <span class="citation">(<a href="#ref-Jaku" role="doc-biblioref">Japkowicz 2000</a>)</span></p>
<p>One approach that is particularly relevant to our work is that of Ling and Li (1998). They combined over-sampling of the minority class with under-sampling of the majorityclass. <span class="citation">(<a href="#ref-ling" role="doc-biblioref">Ling and Li 1998</a>)</span></p>
<p>in order to try and discuss those resampling proceeds, the part “application” shows examples of random up and down sampling in order to make comparison with more sophisticated method as SMOTE and ROSE.</p>
<div id="random-up-and-down-sample" class="section level3" number="4.1.1">
<h3><span class="header-section-number">4.1.1</span> Random Up and Down sample :</h3>
<p>DownSizing will randomly sample a data set so that all classes have the same frequency as the minority class. UpSample samples with replacement to make the class distributions equal.</p>
</div>
<div id="rose" class="section level3" number="4.1.2">
<h3><span class="header-section-number">4.1.2</span> ROSE</h3>
<p>Creates a sample of synthetic data by enlarging the features space of minority and majority class examples. Operationally, the new examples are down from a conditionnal kernel density estimate of the two classes as describe in Menardi and torelli (2013).</p>
<p>The ROSE strategy to deal with class imbalance ROSE <span class="citation">(<a href="#ref-ROSE" role="doc-biblioref">Menardi and Torelli 2014</a>)</span> provides a unified framework to deal simultaneously with the two above-mentioned problems of model estimation and accuracy evaluation in imbalanced learning. It builds on the generation of new artificial examples from the classes, according to a smoothed bootstrap approach.
Consider a training set <span class="math inline">\(T_n\)</span>, of size <span class="math inline">\(n\)</span>, whose generic row is the pair <span class="math inline">\((x_i, y_i)\)</span>, i = 1, . . . , n. The class
labels <span class="math inline">\(y_i\)</span> belong to the set <span class="math inline">\((\gamma0, \gamma1)\)</span>, and <span class="math inline">\(x_i\)</span> are some related attributes supposed to be realizations of a random vector x defined on <span class="math inline">\(R^d\)</span>, with an unknown probability density function <span class="math inline">\(f (x)\)</span>. Let the number of units in class <span class="math inline">\(\gamma_j, j = 0, 1,\)</span> be denoted by <span class="math inline">\(n_j &lt; n\)</span>. The ROSE procedure for generating one new artificial
example consists of the following steps:</p>
<ul>
<li><ol style="list-style-type: decimal">
<li>Select <span class="math inline">\(y* = Y_j\)</span> with probability <span class="math inline">\(\pi_j\)</span>.</li>
</ol></li>
<li><ol start="2" style="list-style-type: decimal">
<li>Select $(xi, yi) T_n, <span class="math inline">\(such that\)</span>y_i = y*$, with probability <span class="math inline">\(\frac{1}{n_j}\)</span>.</li>
</ol></li>
<li><ol start="3" style="list-style-type: decimal">
<li>Sample <span class="math inline">\(x^*\)</span> from <span class="math inline">\(K_{H_j} (.,xi)\)</span>, with <span class="math inline">\(K_{H_j}\)</span> a probability distribution centered at <span class="math inline">\(xi\)</span> and covariance matrix <span class="math inline">\(Hj\)</span>.</li>
</ol></li>
</ul>
<p>Essentially, we draw from the training set an observation belonging to one of the two classes, and generate a new example <span class="math inline">\((x*, y*)\)</span> in its neighborhood, where the shape of the neighborhood is determined by the shape of the contour sets of <span class="math inline">\(K\)</span> and its width is governed by <span class="math inline">\(Hj\)</span>.</p>
</div>
<div id="smote-family-smotenc-blsmote-adasyn" class="section level3" number="4.1.3">
<h3><span class="header-section-number">4.1.3</span> Smote family (smoteNC, BLSMOTE, ADASYN)</h3>
<p>SMOTE is a well-known algorithm to fight this problem. The general idea of this method is to artificially generate new examples of the minority class using the nearest neighbors of these cases. Furthermore, the majority class examples are also under-sampled, leading to a more balanced dataset.</p>
<p>The SMOTE function oversamples your rare event by using bootstrapping and k-nearest neighbor to synthetically create additional observations of that event. The definition of rare event is usually attributed to any outcome variable that happens less than 15% of the time.</p>
<p>The minority class is over-sampled by taking each minority class sample and introducing synthetic examples along the line segments joining any/all of the k minority class nearest neighbors. Depending upon the amount of over-sampling required, neighbors from the k nearest neighbors are randomly chosen. The implementation currently uses five nearest neighbors. For instance, if the amount of over-sampling needed is 200%, only two neighbors from the five nearest neighbors are chosen and one sample is generated in the direction of each. Synthetic samples are generated in the following way: Take the difference between the feature vector (sample) under consideration and its nearest neighbor. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general. <span class="citation">(<a href="#ref-chawla2002" role="doc-biblioref">chawla 2002</a>)</span></p>
<p><img src="smote.png" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="learning-method-tuning" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Learning method tuning</h2>
<div id="metaparameters-tuning" class="section level3" number="4.2.1">
<h3><span class="header-section-number">4.2.1</span> Metaparameters tuning</h3>
<p>A first try can be to handle the parameters of the classifiers used. Empirycally we’ll see in part 5 “Application” that it don’t give some significant results to counteract imbalanced data set predictions.</p>
</div>
<div id="weighted-class" class="section level3" number="4.2.2">
<h3><span class="header-section-number">4.2.2</span> Weighted class</h3>
<p>We try to attribute some weights to our class but the results was not convinced. We try different ways and look for serious or significant results with this methods but it seems that it don’t give results. Maybe the algorithm incorporing these results must be reshaped.</p>
<p>The best try to handle with classifiers during the learning step was to incorporate a cost matrix. We choose to use it with decision tree because the algorithm CART shows good results in the literrature.<span class="citation">(<a href="#ref-MK" role="doc-biblioref">Kuhn and Jonshon 2013</a>)</span></p>
</div>
<div id="direct-sensitive-learning-with-cart-classifer" class="section level3" number="4.2.3">
<h3><span class="header-section-number">4.2.3</span> direct sensitive learning with cart classifer</h3>
<p>The incorporation of beneﬁts and/or costs (negative beneﬁts) in existing algorithms, as a way to express the utility of diﬀerent predictions, is one of the known approaches to cope with imbalanced domains.</p>
<p>Instead of optimizing the typical performance measure, such as accuracy or impurity, some models can alternatively optimize a cost or loss function that deferentially weights speciﬁc types of errors.</p>
<p>We summarize the theory of cost-sensitive learning, published mostly in (Elkan, 2001). The theory describes how the misclassification cost plays its essential role invarious cost-sensitive learning algorithms.</p>
<p>A cost matrix assigns a cost to each cell in the confusion matrix.
The example below is a cost matrix where we use the notation C() to indicate the cost, the first value represented as the predicted class and the second value represents the actual class. <span class="citation">(<a href="#ref-elkanCS" role="doc-biblioref">Elkan 2002</a>)</span></p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="1">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Predicted
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
</th>
<th style="text-align:left;">
Positive
</th>
<th style="text-align:left;">
Negative
</th>
</tr>
</thead>
<tbody>
<tr grouplength="2">
<td colspan="3" style="border-bottom: 1px solid;">
<strong>True</strong>
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
Positive
</td>
<td style="text-align:left;">
C(1, 1) TP
</td>
<td style="text-align:left;">
C(1,0) FN
</td>
</tr>
<tr>
<td style="text-align:left;padding-left: 2em;" indentlevel="1">
Negative
</td>
<td style="text-align:left;">
C(0, 1) FP
</td>
<td style="text-align:left;">
C(0,0) TN
</td>
</tr>
</tbody>
</table>
<p>We can see that the cost of a False Positive is C(1,0) and the cost of a False Negative is C(0,1).</p>
<p>An intuition from this matrix is that the cost of misclassification is always higher than correct classification, otherwise, cost can be minimized by predicting one class.</p>
<p>Conceptually, the cost of labeling an example incorrectly should always be greater than the cost of labeling it correctly.</p>
<p><span class="math display">\[Total Cost = C(0,1) * False Negatives + C(1,0) * False Positives\]</span>
This is the value that we seek to minimize in cost-sensitive learning, at least conceptually.</p>
<p>To go further, a very good paper here. <span class="citation">(<a href="#ref-lingCS" role="doc-biblioref">C. X. Ling and Sheng 2008</a>)</span></p>
</div>
</div>
<div id="post-processing-threesholding" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> post-processing threesholding</h2>
<p>When there are two possible outcome categories, another method for increasing the prediction accuracy of the minority class samples is to determine alternative cutoﬀs for the predicted probabilities</p>
<p>The threshold probability (kind of classification criteria): <span class="math display">\[ P(Y=1, X=x) &gt; 0,5\]</span></p>
<p>0.5 is a basic threshold which means that objects predicted with a probability to belong class 1 is higher than 0.5 will be classify as positive class. We can, for example, be more selective in terms of entry criteria and choose a threshold at 0,7.
There may be situations where the sensitivity/speciﬁcity trade-oﬀ can be accomplished without severely compromising the accuracy of the majority class (which, of course, depends on the context of the problem).</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-chawla2002" class="csl-entry">
chawla. 2002. <em>SMOTE: Synthetic Minority over-Sampling Technique</em>. Journal or Artificial Intelligence Research.
</div>
<div id="ref-elkanCS" class="csl-entry">
Elkan, Charles. 2002. <em>The Foubdation of Cost Sensitive Learning</em>.
</div>
<div id="ref-Jaku" class="csl-entry">
Japkowicz, Nathalie. 2000. <em>Learning from Imbalanced Data Sets: A Comparison of Various Strategies</em>. DalTech/Dalhousie University, 6050 University: The R journal.
</div>
<div id="ref-MK" class="csl-entry">
Kuhn, Max, and Kjell Jonshon. 2013. <em>Applied Predictive Modeling</em>. Springer.
</div>
<div id="ref-lingCS" class="csl-entry">
Ling, Charles X., and Victor S. Sheng. 2008. <em>Cost Sensitive Learning and the Class Imbalance Problem</em>. The university of Western Ontario, Canada: Encyclopedia of Machine Learning.
</div>
<div id="ref-ling" class="csl-entry">
Ling, and and Li. 1998. <em>Data Mining for Direct Marketing: Problems and Solutions</em>. Proceedings of the Fourth.
</div>
<div id="ref-ROSE" class="csl-entry">
Menardi, Nicola Lunardon Giovanna, and Nicola Torelli. 2014. <em>ROSE: A PAckage for Binary Imbalanced</em>. The R journal.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classifiers.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="applications.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Test_book.pdf", "Test_book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
