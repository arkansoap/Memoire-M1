---
title: "spotify"
author: "Thibault FUCHEZ"
date: "04/05/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package

```{r}
library (tidyverse) # collection of packages for data analysis
library(caret) # Classification And REgression Training
library(pROC) # Tools for visualizing, smoothing and comparing ROC.
library(ROCR) # evaluating and visualizing classifier performance
library(lubridate) # R commands for date-times
library(tidymodels) #  collection of packages for modeling and machine learning using tidyverse principles. pre-process/train/validate
library(MASS) # Modern Applied Statistics with S" for regression and classification
library(stargazer) # Well-Formatted Regression and Summary Statistics Tables
library(randomForest) # ensemble learning method with multitude of decision tree 
library(doParallel) # paralléliser le calcul pour que ce soit plus rapide en créant un cluster de cœurs.
library(parallel) # détection du nombre de coeurs
library(ranger) # A Fast Implementation of Random Forests. (tune rf)
library(e1071) # Misc Functions of the Department of Statistics, Probability Theory Group / for svm, tune, predict, ...
library(kableExtra) # Create Awesome HTML Table
library(smotefamily) # Synthetic Minority Oversampling TEchnique
library(ROSE) # Generation of synthetic data by Randomly Over Sampling Examples
library(gdata) # Various R programming tools for data manipulation / rename object with mv("oldname, newname")
library(naivebayes) #naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem
```

```{r}
setwd("C:/Users/arkan/Desktop/Memoire-M1/applications_R")
```

```{r}
source("functions_UC.R")
```
  
# Préparation de la base de données ...

## En amont 

```{r}
#On charge la base de donnée. Les variables de types "str" sont recodées en facteurs.
spotify <- read.csv("spotify_songs.csv", header = TRUE, sep = ",",stringsAsFactors=T)

# Etant données qu'il n'y a que 15 valeurs manquantes, on décide de supprimer les individus contenant ces valeurs.
sum(is.na(spotify))
spotify = na.omit(spotify)

# Lignes dupliquées : 
spot1 <- spotify[!duplicated(spotify$track_id),]
# spot2 <- spotify
# spot2 %>% distinct(track_id)
# spot3 <- spotify[duplicated(spot2$track_id),]
# spot4 <- spotify[!duplicated(spotify$track_name),]

#Changement du nom des levels:
spot1$key <- recode_factor(spot1$key,
  "0" = "C",
  "1" = "Db",
  "2" = "D",
  "3" = "Eb",
  "4" = "E",
  "5" = "F",
  "6" = "Gb",
  "7" = "G",
  "8" = "Ab",
  "9" = "A",
  "10" = "Bb",
  "11" = "B"
)

spot1$mode <- recode_factor(spot1$mode, "0" = "mineur", "1" = "majeur")

# recodage de la date / regroupement par tranches :
spot1$track_album_release_date <- as.Date(spot1$track_album_release_date , format= "%Y")
spot1<- spot1 %>% mutate(year = year(track_album_release_date))
spot1$years <- cut(spot1$year, breaks = c(1956,2000,2010,2015,2018,2020),
                   diag.lab=0,
                   labels = c("1956-2000","2001-2010",
                              "2011-2015","2016-2018","2019-2020"))

# Suppression de instrumentalness mal codé : 
spot1<-spot1[,-c(7,19)]

# Recodage de track-popularity / regroupement basse/autres :
popularity <- cut(spot1$track_popularity, c(-1,12.5,100))
spot_B=cbind(popularity,spot1)
spot_B<-spot_B[,-5]
# spot_B$popularity <- recode_factor(spot_B$popularity,
#                                      "(12.5,100]" = "Autre",
#                                      "(-1,12.5]" = "basse")
spot_B$popularity <- recode_factor(spot_B$popularity,
                                   "(12.5,100]" = 0,
                                   "(-1,12.5]" = 1)


# On enlève les variables non pertinentes pour la classification : 
spot_B<-spot_B[,-c(2,3,4,5,6,7,8,10)]

# On efface les bases inutiles 
rm(spot1, spotify)
```

```{r}
plot(spot_B$popularity)
```

## Partitionnement : 

The training and test sets were created using stratiﬁed random sampling. First, split the training set off. Then create the evaluation and test sets. Hence we normalize/standardize the datas. 

```{r}
#posssibilité de changer les paramètres prop1, prop2, seed1, seed2
datas <- split_standard2(spot_B, "popularity")

# verif
sum(nrow(datas$train),nrow(datas$test),nrow(datas$eval))==nrow(spot_B)
```

```{r}
# Version avec moins de paramètre
# datas <- split_standard(spot_B, "popularity")
```

# First Models : 

```{r}
Models <- models(y = "popularity", data = datas$train)
```

```{r}
Predictions <- predictions(models = Models, datas = datas)
```

```{r}
# performance measure and comparisons
perf.measure(pred = Predictions$predrf, realCl = datas$test$popularity, real = datas$test)
```

# Remedies :

## tune parameters :

### RF

Choix optimal des paramètres avec caret :           

```{r}
# enregistrement d'un cluster avec le package doParallel
library(doParallel)
registerDoParallel(cores = 6)

rangerGrid <- expand.grid(mtry = c(1, 2, 4, 8, 14),
                          min.node.size = c(5, 10, 50, 100),
                          splitrule = "gini"
                          )
ctrlCv <- trainControl(method = "repeatedcv", repeats = 3, number = 5)

system.time(ranger.rf <- train(popularity ~ .,
                              data = datas$train, method = "ranger",
                              trControl = ctrlCv, tuneGrid = rangerGrid
                              )
            )

# fermeture du cluster
stopImplicitCluster()

# pour CV, paramètre number et reapets ?
# na.action not necessary because all na were ommitted
```

```{r}
ranger.rf
ranger.rf$bestTune
ranger.rf$finalModel
```

### SVM

```{r}
registerDoParallel(cores = 6)

system.time(tuneSvm <- tune(svm, popularity~., data = datas$test,
                      ranges = list(gamma = c(0.1, 1, 10), cost = 10^(1:3)),
                      tunecontrol = tune.control(nrepeat = 10,
                                                 sampling = "cross",
                                                 cross = 10)))


stopImplicitCluster()
```

```{r}
tuneSvm
```

## Alternate cut off :

On expliquera le procédé.
En conclusion, on explicitera que cela nous coute trop en terme d'erreur globale si l'on veut un bon pouvoir de détection. 

### LDA 

```{r}
evoSeuil(predLda, datas$test$popularity, datas$test, "autre")
```

```{r}
changeSeuil(predLda, datas$test$popularity, datas$test, 0.2, "autre")
```

### Logit 

```{r}
evoSeuil(problog, datas$train$popularity, datas$train, "logit")
```

```{r}
logseuil<-changeSeuil(problog, datas$train$popularity, datas$train,0.3, "logit")
```

```{r}
perf.measure(predlog, datas$train$popularity, datas$train)
```

## Cost function

## Unequal case weight 

### SVM

```{r}
set.seed(1983)
svmFitW <- svm(popularity ~ ., data = datas$train, 
              scale = FALSE, 
              kernel = "radial", cost = 10,
              class.weights = c("0" = 1, "1" = 4))
```

```{r}
predict.svmW <- predict(svmFitW, newdata = datas$test)
mean(predict.svmW==datas$test$popularity)
confusionMatrix(predict.svmW,datas$test$popularity )
```

## Resampling methods

### Over and down sampling

### SMOTE 

Synthetic Minority Oversampling Technique

need numeric data only : 

```{r}
datas_train_num<- datas$train[, -c(2,5,7,15)]
```

```{r}
smoteData <- SMOTE(datas_train_num[,-1],datas_train_num[,1])
NewdataSmote <- smoteData$data
NewdataSmote$class<-as.factor(NewdataSmote$class)
plot(NewdataSmote$class)
```

### ROSE

Random Over-Sampling Examples

```{r}
rose <- ROSE(popularity~. , spot_B)
RoseData <- rose$data
plot(RoseData$popularity)
```

```{r}
datasRose<- split_standard2(RoseData, "popularity")
```

```{r}
ModelsRose <- models(y = "popularity", data = datasRose$train)
```

```{r}
predRose <- predictions(models = ModelsRose, datas = datasRose )
```
