---
title: "Spotify"
author: "Thibault FUCHEZ"
date: "01/06/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package

```{r}
library (tidyverse) # collection of packages for data analysis
library(caret) # Classification And REgression Training
library(pROC) # Tools for visualizing, smoothing and comparing ROC.
library(ROCR) # evaluating and visualizing classifier performance
library(lubridate) # R commands for date-times
library(tidymodels) #  collection of packages for modeling and machine learning using tidyverse principles. pre-process/train/validate
library(MASS) # Modern Applied Statistics with S" for regression and classification
library(stargazer) # Well-Formatted Regression and Summary Statistics Tables
library(randomForest) # ensemble learning method with multitude of decision tree 
library(doParallel) # paralléliser le calcul pour que ce soit plus rapide en créant un cluster de cœurs.
library(parallel) # détection du nombre de coeurs
library(ranger) # A Fast Implementation of Random Forests. (tune rf)
library(e1071) # Misc Functions of the Department of Statistics, Probability Theory Group / for svm, tune, predict, ...
library(kableExtra) # Create Awesome HTML Table
library(smotefamily) # Synthetic Minority Oversampling TEchnique
library(ROSE) # Generation of synthetic data by Randomly Over Sampling Examples
library(gdata) # Various R programming tools for data manipulation / rename object with mv("oldname, newname")
library(naivebayes) #naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem
library(glmnet) # Lasso and Elastic-Net Regularized Generalized Linear Models
```

```{r}
setwd("C:/Users/arkan/Desktop/Memoire-M1/applications_R")
```

```{r}
source("functions_UC.R")
```
  
# Préparation de la base de données ...

## En amont 

```{r}
#On charge la base de donnée. Les variables de types "str" sont recodées en facteurs.
spotify <-
  read.csv(
    "spotify_songs.csv",
    header = TRUE,
    sep = ",",
    stringsAsFactors = T
  )

# Etant données qu'il n'y a que 15 valeurs manquantes, on décide de supprimer les individus contenant ces valeurs.
sum(is.na(spotify))
spotify = na.omit(spotify)

# Lignes dupliquées :
spot1 <- spotify[!duplicated(spotify$track_id), ]
# spot2 <- spotify
# spot2 %>% distinct(track_id)
# spot3 <- spotify[duplicated(spot2$track_id),]
# spot4 <- spotify[!duplicated(spotify$track_name),]

#Changement du nom des levels:
spot1$key <- recode_factor(
  spot1$key,
  "0" = "C",
  "1" = "Db",
  "2" = "D",
  "3" = "Eb",
  "4" = "E",
  "5" = "F",
  "6" = "Gb",
  "7" = "G",
  "8" = "Ab",
  "9" = "A",
  "10" = "Bb",
  "11" = "B"
)

spot1$mode <-
  recode_factor(spot1$mode, "0" = "mineur", "1" = "majeur")

# recodage de la date / regroupement par tranches :
spot1$track_album_release_date <-
  as.Date(spot1$track_album_release_date , format = "%Y")
spot1 <- spot1 %>% mutate(year = year(track_album_release_date))
spot1$years <-
  cut(
    spot1$year,
    breaks = c(1956, 2000, 2010, 2015, 2018, 2020),
    diag.lab = 0,
    labels = c(
      "1956-2000",
      "2001-2010",
      "2011-2015",
      "2016-2018",
      "2019-2020"
    )
  )

# Suppression de instrumentalness mal codé :
spot1 <- spot1[, -c(7, 19)]

# Recodage de track-popularity / regroupement basse/autres :
popularity <- cut(spot1$track_popularity, c(-1, 12.5, 100))
spot_B = cbind(popularity, spot1)
spot_B <- spot_B[, -5]
# spot_B$popularity <- recode_factor(spot_B$popularity,
#                                      "(12.5,100]" = "Autre",
#                                      "(-1,12.5]" = "basse")
spot_B$popularity <- recode_factor(spot_B$popularity,
                                   "(12.5,100]" = 0,
                                   "(-1,12.5]" = 1)


# On enlève les variables non pertinentes pour la classification :
spot_B <- spot_B[, -c(2, 3, 4, 5, 6, 7, 8, 10)]

# On efface les bases inutiles
rm(spot1, spotify)
```

```{r}
plot(spot_B$popularity)
```

## Partitionnement : 

The training and test sets were created using stratiﬁed random sampling. First, split the training set off. Then create the evaluation and test sets. Hence we normalize/standardize the datas. 

```{r}
#posssibilité de changer les paramètres prop1, prop2, seed1, seed2
datas <- split_standard(spot_B, "popularity", mod = "standard")

# verif
sum(nrow(datas$train),nrow(datas$test),nrow(datas$eval))==nrow(spot_B)
```

# First Models : 

```{r}
Models <- models(y = "popularity", data = datas$train)
```

```{r}
Predictions <- predictions(models = Models, datas = datas)
```

```{r}
kab1 <- KablesPerf(pred = Predictions, dat = datas$test, y = "popularity")

kab1[[1]]
kab1[[2]]
```

```{r}
AllRoc(Predictions, datas$test$popularity)
```

We can emphasize the low efficiency of accuracy when we faces imblanced sample. Hence, depsite a very good accuracy, we can see than false alarm and detection power are not good. 

# Remedies 

## Datas prepocessing : 

### Random downsample

```{r}
DownSampleRandom <- downSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(DownSampleRandom$Class)
```

### Random upsample

```{r}
UpSampleRandom <- upSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(UpSampleRandom$Class)
```

### SMOTE 

Synthetic Minority Oversampling Technique

need numeric data only : 

```{r}
datas_num<- spot_B[, -c(2,5,7,15)]
```

```{r}
smote <- SMOTE(datas_num[,-1],datas_num[,1])
```

```{r}
Smotedata <- smote$data
Smotedata$class<-as.factor(Smotedata$class)
str(Smotedata)
plot(Smotedata$class)
Smotedata$popularity <- Smotedata$class
Smotedata <- Smotedata[,-11]
```

```{r}
datasSmote <- split_standard(Smotedata, "popularity")
```

```{r}
ModelsSmote <- models(y = "popularity", data = datasSmote$train)
```

```{r}
predSmote <- predictions(models = ModelsSmote, datas = datasSmote)
```

```{r}
listpredSmote <- list(predSmote$predrf$cla, predSmote$predlog$cla,
                      predSmote$predSvm, predSmote$predLda$class)
lapply(listpredSmote, perf.measure,
       real = datasSmote$test,
       y = "popularity")
```

### BLSmote

### ADASYN

### ROSE

Random Over-Sampling Examples

rose.eval ...

```{r}
rose <- ROSE(popularity~. , spot_B)
RoseData <- rose$data
plot(RoseData$popularity)
```

```{r}
datasRose<- split_standard(RoseData, "popularity")
```

```{r}
ModelsRose <- models(y = "popularity", data = datasRose$train)
```

```{r}
predRose <- predictions(models = ModelsRose, datas = datasRose)
```

```{r}
listpredRose <- list(predRose$predrf$cla, predRose$predlog$cla, predRose$predSvm, predRose$predLda$class)
lapply(listpredRose, perf.measure,
       real = datasRose$test,
       y = "popularity")
```

## Learning method tuning : 

Note for svm : 
  - weight class parameter
  - radial or polynomial, ...
  - cost : The cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.

### Tune parameter with no direct link with imbalanced data : 

*RF

```{r}
# enregistrement d'un cluster avec le package doParallel
library(doParallel)
registerDoParallel(cores = 6)

rangerGrid <- expand.grid(
  mtry = c(1, 2, 4, 8, 14),
  min.node.size = c(5, 10, 50, 100),
  splitrule = "gini"
)
ctrlCv <-
  trainControl(method = "repeatedcv",
               repeats = 3,
               number = 5)

system.time(
  ranger.rf <- train(
    popularity ~ .,
    data = datas$train,
    method = "ranger",
    trControl = ctrlCv,
    tuneGrid = rangerGrid
  )
)

# fermeture du cluster
stopImplicitCluster()

# pour CV, paramètre number et reapets ?
# na.action not necessary because all na were ommitted
```

```{r}
ranger.rf
ranger.rf$bestTune
ranger.rf$finalModel
```

* SVM

```{r}
registerDoParallel(cores = 6)

system.time(
  tuneSvm <- tune(
    svm,
    popularity ~ .,
    data = datas$test,
    ranges = list(gamma = c(0.1, 1, 10), cost = 10 ^ (1:3)),
    tunecontrol = tune.control(
      nrepeat = 10,
      sampling = "cross",
      cross = 10
    )
  )
)


stopImplicitCluster()
```

```{r}
tuneSvm
```

### Tune Class weight 

* SVM

```{r}
set.seed(1983)
svmFitW <- svm(
  popularity ~ .,
  data = datas$train,
  scale = FALSE,
  kernel = "radial",
  cost = 10,
  class.weights = c("0" = 1, "1" = 4)
)
```

```{r}
predict.svmW <- predict(svmFitW, newdata = datas$test)
mean(predict.svmW==datas$test$popularity)
confusionMatrix(predict.svmW,datas$test$popularity )
```

* Faire pour 3 autres model

## Prediction postprocessing : 

```{r}
predbasic <- predictions(models = Models, datas = datas)
```

* alternate cutoff LDA

```{r}
evoSeuil(Predictions$predLda, datas$test$popularity, datas$test, "autre")
```

```{r}
predldaseuil <- changeSeuil(predbasic$predLda, datas$test$popularity, datas$test, 0.2, "autre")
```

```{r}
perf.measure(predldaseuil, datas$test$popularity, datas$test)
```

* alternate cut off logit

```{r}
evoSeuil(predbasic$predlog$prob, datas$test$popularity, datas$test, "logit")
```

```{r}
predlogitseuil <- changeSeuil(predbasic$predlog$prob, datas$test$popularity, datas$test, 0.2, "logit")
```

```{r}
perf.measure(predlogitseuil, datas$test$popularity, datas$test)
```

* alternate cutoff rf

```{r}
evoSeuil(pred = predbasic$predrf$prob[,2], datas$test$popularity, datas$test, "logit")
```

```{r}
predrfseuil <- changeSeuil(predbasic$predrf$prob[,2], datas$test$popularity, datas$test, 0.2, "logit")
```

```{r}
perf.measure(predrfseuil, datas$test$popularity, datas$test)
```

*alternate cutoff svm

Changer fonction predictions pour sortir les prob de svm si possible 

```{r}
svmtest <- svm(
    popularity~.,
    data = datas$train,
    scale = FALSE,
    kernel = "polynomial",
    cost = 5,
    probability = TRUE
)
```

```{r}
testprobsvm <- predict(Models$ModSvm, newdata = datas$test, probability = TRUE)
```

```{r}
AA <- attr(testprobsvm, "probabilities")[,2]
```

```{r}
evoSeuil(AA, datas$test$popularity, datas$test, "logit")
```

```{r}
AA2 <- changeSeuil(pred = AA, datas$test$popularity, datas$test, 0.19, "logit" )
```

```{r}
perf.measure2(AA2, datas$test$popularity, datas$test)
```

```{r}
perf.measure(AA2, datas$test, "popularity")
```


# Combination of the three remedies step for the best model

# Notes / Tests/ à voir pr correction

```{r}
# Avec CARET
confusionMatrix(predLda$class, datas[["test"]]$popularity)
```

```{r}
# Avec proc 

# Pourquoi pas la même qu'au dessus ?????????????
# control and case group ????? (rapport avec réalité et prédiction / levels?)
rocLDA <- roc(datas[["test"]]$popularity, ordered(predLda$class))
plot(rocLDA)
rocLDA$auc
```

```{r}
Dat <- split_standard(spot_B, "popularity")
```

```{r}
source("functions_UC.R")
```

```{r}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "gini",
  .min.node.size = 5
)

testMod <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = tuneGrid)
```

```{r}
testMod2 <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = NULL,
                 tuneLength = 1 ) 
```

```{r}
# models2 <- function(y, data, tuneLenght = 1, tuneGridglm = NULL,
#                     costSvm = 1, kernelSv = "polynomial",
#                     weights = c(0.5,0.5)){
#   Modlda <- train(popularity~.,
#                   data = data,
#                   method = "LDA",
#                   tuneLenght = tuneLenght
#                   )
#   Modlr <- train(popularity~.,
#                  method = "glmnet",
#                  tuneLenght = tuneLenght,
#                  tuneGrid = tuneGridglm
#                  )
#   modSvm <- train(popularity~.,
#                   method = "svm",
#                   probability = TRUE,
#                   cost = costSvm,
#                   kernel = kernelSvm
#                   )
# }
```

```{r}
length(datas$train) - 1
```

weigths pour logit : need pour chaque observation!!! different de classweight

