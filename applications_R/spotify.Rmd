---
title: "Spotify"
author: "Thibault FUCHEZ"
date: "01/06/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
setwd("C:/Users/arkan/Desktop/Memoire-M1/applications_R")
```

```{r}
source("functions_UC.R")
```
  
# Preparation of the data base

## restructuring 

```{r}
#On charge la base de donnée. Les variables de types "str" sont recodées en facteurs.
spotify <-
  read.csv(
    "spotify_songs.csv",
    header = TRUE,
    sep = ",",
    stringsAsFactors = T
  )

# Etant données qu'il n'y a que 15 valeurs manquantes, on décide de supprimer les individus contenant ces valeurs.
sum(is.na(spotify))
spotify = na.omit(spotify)

# Lignes dupliquées :
spot1 <- spotify[!duplicated(spotify$track_id), ]
# spot2 <- spotify
# spot2 %>% distinct(track_id)
# spot3 <- spotify[duplicated(spot2$track_id),]
# spot4 <- spotify[!duplicated(spotify$track_name),]

#Changement du nom des levels:
spot1$key <- recode_factor(
  spot1$key,
  "0" = "C",
  "1" = "Db",
  "2" = "D",
  "3" = "Eb",
  "4" = "E",
  "5" = "F",
  "6" = "Gb",
  "7" = "G",
  "8" = "Ab",
  "9" = "A",
  "10" = "Bb",
  "11" = "B"
)

spot1$mode <-
  recode_factor(spot1$mode, "0" = "mineur", "1" = "majeur")

# recodage de la date / regroupement par tranches :
spot1$track_album_release_date <-
  as.Date(spot1$track_album_release_date , format = "%Y")
spot1 <- spot1 %>% mutate(year = year(track_album_release_date))
spot1$years <-
  cut(
    spot1$year,
    breaks = c(1956, 2000, 2010, 2015, 2018, 2020),
    diag.lab = 0,
    labels = c(
      "1956-2000",
      "2001-2010",
      "2011-2015",
      "2016-2018",
      "2019-2020"
    )
  )

# Suppression de instrumentalness mal codé :
spot1 <- spot1[, -c(7, 19)]

# Recodage de track-popularity / regroupement basse/autres :
popularity <- cut(spot1$track_popularity, c(-1, 12.5, 100))
spot_B = cbind(popularity, spot1)
spot_B <- spot_B[, -5]
# spot_B$popularity <- recode_factor(spot_B$popularity,
#                                      "(12.5,100]" = "Autre",
#                                      "(-1,12.5]" = "basse")
spot_B$popularity <- recode_factor(spot_B$popularity,
                                   "(12.5,100]" = 0,
                                   "(-1,12.5]" = 1)


# On enlève les variables non pertinentes pour la classification :
spot_B <- spot_B[, -c(2, 3, 4, 5, 6, 7, 8, 10,23)]

# On efface les bases inutiles
rm(spot1, spotify)
```

```{r}
plot(spot_B$popularity)
```

We can observe a high imbalanced repartition on the "popularity" variable.

## Partitioning : 

Training and test sets are created with random sampling. First, we split the training set off. Then whe create the evaluation and test sets. Hence we create two groups of datas, one which is standardize and the other which is not. You can see the functions used in the functions_UCR.R file.

```{r}
#posssibilité de changer les paramètres prop1, prop2, seed1, seed2
datas <- split_standard(spot_B, "popularity", mod = "standard")
datasNS <- split_standard(spot_B, "popularity", mod = "nonstandard")
# verif
sum(nrow(datas$train),nrow(datas$test),nrow(datas$eval))==nrow(spot_B)
```

# First Models : 

To introduce our problematics, we fit four kind of models withs basics parameters. We choose LDA, RF, LR and SVM (a brief overview of each model is available in the summary document).

The priors of the training set are :

```{r}
priors(dat = datas$train, "popularity")
```

```{r}
Models <- models(y = "popularity", data = datas$train,
                 prior = priors(dat = datas$train, "popularity"))
# ModelsNS <- models(y = "popularity", data = datasNS$train)
```

With current parameter, The SVM model fails to find an hyperplane with non normalize datas. This errors occurs when whe faces imbalance data. 

```{r}
Predictions <- predictions(models = Models, datas = datas)
```

```{r}
kab1 <- KablesPerf(pred = Predictions, dat = datas$test, y = "popularity")

kab1[[1]]
kab1[[2]]
```

```{r}
par(mfrow=c(1,1))
AllRoc(Predictions, datas$test$popularity)
```

We can emphasize the low efficiency of accuracy when we faces imblanced sample. Hence, depsite a very good accuracy, we can see than false alarm and detection power are very bad. 
Note that RF has pretty good results in comparison of the other models.
We clearly observe that our models are inefficient. Hence, because of the small size of the positive class, the models tends to classify almost all individuals in the minority class (not enough datas to learn about positive class structure). Linked to the known fact that there is a large majority of individuals in the minority level, We can understand the very good score of accuracy.

# Remedies 

A lot of research have been made concerning this problem. Our goal is not to make an exhaustive review of all the technics to remedies this issue. 
In this study, we will focus on methods than we can repoduce with our level of competence. It appears to us that it is interesting to separate the choosen methods in three levels :
- First, some remedies we can use before launching the machine learning algorithm (Preprocessing).
- Secundly, some remedies we can use during the computation of a fitted models by the machine (learning method tuning).
- At last, som remedies that can be used after the machine learning algorithm (postprocessing).

## Datas prepocessing : 


### CARET up and down sample : 

"downSample will randomly sample a data set so that all classes have the same frequency as the minority class. upSample samples with replacement to make the class distributions equal"

#### Random downsample

```{r}
DownSampleRandom <- downSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(DownSampleRandom$Class)
```

```{r}
datasRDS <- split_standard(DownSampleRandom, "Class", mod = "standard")
```

```{r}
ModelsRDS <- models(y = "Class", data = datasRDS$train,
                    prior = priors(datasRDS$train, "Class"))
```

```{r}
PredictionsRDS <- predictions(models = ModelsRDS, datas = datasRDS)
```

```{r}
KablesPerf(pred = PredictionsRDS, dat = datasRDS$test, y = "Class")
```

```{r}
AllRoc(predic = PredictionsRDS, dataCl = datasRDS$test$Class)
```

A first try with using Random down-resampling. Here we randomly reduce the large class (negative class). The major default of this method is that we significantly reduces the data base which one the algorithm learn. We easily understand than the less cases the machine has, the worst reliable the predictions are.

RF is still the best model, following by SVM. 

A major issue is the False alarm rate (FP rate). 

To get a better compromise beetween sensitivity(TPrate) and specificity(TNrate), we need to pay it in glabal accuracy. It cost in accuracy (and False alarm) to gain a better detection power. 

However, Results are globally better than with the models without resampling.

#### Random upsample

```{r}
UpSampleRandom <- upSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(UpSampleRandom$Class)
```

```{r}
datasRUS <- split_standard(UpSampleRandom, "Class", mod = "standard")
```

```{r}
ModelsRUS <- models(y = "Class", data = datasRUS$train,
                    prior = priors(datasRUS$train, "Class"))
```

```{r}
PredictionsRUS <- predictions(models = ModelsRUS, datas = datasRUS)
```

```{r}
KablesPerf(pred = PredictionsRUS, dat = datasRUS$test, y = "Class")
```

```{r}
AllRoc(predic = PredictionsRUS, dataCl = datasRUS$test$Class)
```

Excellent results for random forest !! How can we explain it ? Maybe because we standardize the data base?

```{r}
testA <- resamp_res(datas = spot_B, mod = "US")
```

### SMOTE 

Synthetic Minority Oversampling Technique

need numeric data only : 

```{r}
datas_num<- spot_B[, -c(2,5,7)]
```

```{r}
Smote <- SMOTE(datas_num[,-1],datas_num[,1])
```

```{r}
Smotedata <- Smote$data
Smotedata$class<-as.factor(Smotedata$class)
```

```{r}
plot(Smotedata$class)
```

```{r}
datasSmote <- split_standard(Smotedata, "class", mod = "standard")
```

```{r}
ModelsSmote <- models(y = "class", data = datasSmote$train,
                      prior = priors(datasSmote$train, "class"))
```

```{r}
predSmote <- predictions(models = ModelsSmote, datas = datasSmote)
```

```{r}
KablesPerf(pred = predSmote, dat = datasSmote$test, y = "class")
```

```{r}
AllRoc(predic = predSmote, dataCl = datasSmote$test$class)
```

We are reaching the same conclusion than upsampling. The actual results are less "extreme". Possibly due to the fact that RUS create too much individuals and just make duplications of cases.  
Develop ... 

### BLSmote

```{r}
BLSmote <- BLSMOTE(datas_num[,-1],datas_num[,1])
```

```{r}
BLSmotedata <- BLSmote$data
BLSmotedata$class<-as.factor(BLSmotedata$class)
datasBLSmote <- split_standard(BLSmotedata, "class", mod = "standard")
```

```{r}
ModelsBLSmote <- models(data = datasBLSmote$train, y = "class",
       prior = priors(datasBLSmote$train, "class"))
```

```{r}
PredicionsBLSmote <- predictions(ModelsBLSmote, datasBLSmote )
```

```{r}
KablesPerf(pred = PredicionsBLSmote, dat = datasBLSmote$test, y = "class")
```

```{r}
AllRoc(predic = PredicionsBLSmote, dataCl = datasBLSmote$test$class)
```

### ADASYN

note : faire une fonction pour éviter de se retaper tout le code ...

```{r}
Adasyn <- ADAS(datas_num[,-1],datas_num[,1])
```

```{r}
AdasData <- Adasyn$data
AdasData$class<-as.factor(AdasData$class)
names(AdasData)[names(AdasData) == "class"] <- "Class"
```

```{r}
testB <- resamp_res(datas_num, mod = "Adasyn")
testB[[2]]
```


### ROSE

Random Over-Sampling Examples

note : look more about rose.eval function.

```{r}
rose <- ROSE(popularity~. , spot_B)
RoseData <- rose$data
```

```{r}
plot(RoseData$popularity)
```

```{r}
datasRose<- split_standard(RoseData, "popularity", mod = "standard")
```

```{r}
ModelsRose <- models(y = "popularity", data = datasRose$train,
                     prior = priors(datasRose$train, "popularity"))
```

```{r}
predRose <- predictions(models = ModelsRose, datas = datasRose)
```

```{r}
KablesPerf(predRose, datasRose$test, "popularity")
```

```{r}
AllRoc(predRose, dataCl = datasRose$test$popularity)
```

## Learning method parameter : 

### Tune parameter with no direct link with imbalanced data : 

First, we tune parameters with no direct link with imbalanced datas. tuning these may be make us win performance and relut better metrics. 

* RF

```{r}
# enregistrement d'un cluster avec le package doParallel
library(doParallel)
registerDoParallel(cores = 6)

rangerGrid <- expand.grid(
  mtry = c(1, 2, 4, 8, 12),
  min.node.size = c(5, 10, 50, 100),
  splitrule = "gini"
)
ctrlCv <-
  trainControl(method = "repeatedcv",
               repeats = 3,
               number = 5)

system.time(
  ranger.rf <- train(
    popularity ~ .,
    data = datas$train,
    method = "ranger",
    trControl = ctrlCv,
    tuneGrid = rangerGrid
  )
)

# fermeture du cluster
stopImplicitCluster()

# pour CV, paramètre number et reapets ?
# na.action not necessary because all na were ommitted
```

```{r}
ranger.rf
ranger.rf$bestTune
ranger.rf$finalModel
```

* lr : 

- Remove non significative parameter
- try glmnet to compromise betwwen lasso and ridge regression

* SVM

note : choice of the better kernel

```{r}
registerDoParallel(cores = 6)

system.time(
  tuneSvm <- tune(
    svm,
    popularity ~ .,
    data = datas$test,
    ranges = list(gamma = c(0.1, 1, 10), cost = c(1, 5, 10, 30)),
    tunecontrol = tune.control(
      nrepeat = 1,
      sampling = "cross",
      cross = 10
    )
  )
)


stopImplicitCluster()
```

```{r}
tuneSvm
```

* lda

note : interet de changer le prior ? , préférer qda ?


### cost sensitive learning : 

#### Direct cost sensitive learning (ICET and cart) : CART

```{r}
library(C50)
```

```{r}
priorSpot <- priors(datas$train, "popularity")
Ratio <- priorSpot[1]/priorSpot[2]
```

```{r}
c5Matrix <- matrix(c(0, 1, Ratio, 0), ncol = 2)
rownames(c5Matrix) <- levels(datas$train$popularity)
colnames(c5Matrix) <- levels(datas$train$popularity)
c5Matrix
```

```{r}
modelC5 <- C5.0(popularity~.,
                data = datas$train,
                costs = c5Matrix)
```

```{r}
# PredC5 <- predict(modelC5, datas$test, type  = "prob")
```

Erreur : confidence values (i.e. class probabilities) should 
           not be used with costs
           
```{r}
PredC5b <- predict(modelC5, datas$test)
```


```{r}
perf.measure(PredC5, datas$test, "popularity")
```

```{r}
# function with ratio range from 1 to 20 

Perfs <- NULL
resTP <- NULL
resFP <- NULL
resKAP <- NULL
for (Ratio in 1:20){
  c5Matrix <- matrix(c(0, 1, Ratio, 0), ncol = 2)
  rownames(c5Matrix) <- levels(datas$train$popularity)
  colnames(c5Matrix) <- levels(datas$train$popularity)
  modelC5 <- C5.0(popularity~.,
              data = datas$train,
              costs = c5Matrix)
  PredC5 <- predict(modelC5, datas$test)
  Perfs <- perf.measure(PredC5, datas$test, "popularity")
  resTP <- rbind(resTP,Perfs$TPrate)
  resFP <- rbind(resFP,Perfs$FPrate)
  resTN <- 1-resFP
  resKAP <- rbind(resKAP, Perfs$kappa)
}

par(mfrow=c(1,2))
plot(resTP, xlim = c(0, 20), ylim = c(0, 1),
     ylab = "", xlab = "cost", type = "b",
     col = "blue", pch = 4, lty = 2)
par(new = TRUE)
plot(resTN,  xlim = c(0, 20), ylim = c(0, 1),
     ylab = "", xlab = "",
     col = "red", pch = 3, type = "b")
plot(resKAP,  xlim = c(0, 20), ylim = c(-1, 1))

```

```{r}
C5graph <- function(data, y){
  Perfs <- NULL
  resTP <- NULL
  resFP <- NULL
  resKAP <- NULL
  for (Ratio in 1:20){
    c5Matrix <- matrix(c(0, 1, Ratio, 0), ncol = 2)
    rownames(c5Matrix) <- levels(data$train[[y]])
    colnames(c5Matrix) <- levels(data$train[[y]])
    modelC5 <- C5.0(as.formula(paste(y , "~ .")),
                data = data$train,
                costs = c5Matrix)
    PredC5 <- predict(modelC5, data$test)
    Perfs <- perf.measure(PredC5, data$test, y)
    resTP <- rbind(resTP,Perfs$TPrate)
    resFP <- rbind(resFP,Perfs$FPrate)
    resTN <- 1-resFP
    resKAP <- rbind(resKAP, Perfs$kappa)
  }
  
  par(mfrow=c(1,2))
  plot(resTP, xlim = c(0, 20), ylim = c(0, 1),
       ylab = "", xlab = "cost", type = "b",
       col = "blue", pch = 4, lty = 2)
  par(new = TRUE)
  plot(resTN,  xlim = c(0, 20), ylim = c(0, 1),
       ylab = "", xlab = "",
       col = "red", pch = 3, type = "b")
  plot(resKAP,  xlim = c(0, 20), ylim = c(-1, 1))
}
```

```{r}
C5graph(datas, "popularity")
```

#### Meta learning : weighting


We can decide to give some weights to the data class in order to give more importance (more weight) to the positive class. Link with cost / benefits. !!!! Class weight argument is the way to handle with cost sensistive

note : weights for glm seems to ask a vector of weights for each classes

```{r}
priors(datas$train, "popularity")
```

```{r}
ModW <- models(y = "popularity", data = datas$train,
               prior = c(0.5,0.5),
               CWSvm = c("0" = 1, "1" = 4),
               kernSvm = "radial",
               CWRf = c(1, 4))
```

```{r}
PredW <- predictions(models = ModW, datas = datas)
```

```{r}
KablesPerf(PredW, datas$test, "popularity")
```

Ca baisse un peu le taux de fausse alarme mais augmente le diminue également le taux de detection power.

```{r}
par(mfrow=c(1,2))
AllRoc(Predictions, datas$test$popularity)
AllRoc(PredW, datas$test$popularity)
```

```{r}
# long .....
TestCostSpot <- train(popularity~.,
                      data = datas$train,
                      method = "svmRadial",
                      class.wieghts = c("0" = 1, "1" = 4)
                      )
```


pour deux classes, poids de 1 pour la grande classe et du ratio
des tailles de classes (grande sur petite) pour la petite classe

Pour les paramètres CW de rf et svm, J'ai essayé plein de configuration mais aucune ne donne de résultats.

Changer les prior pour lda améliore les prévisions mais le kappa reste assez faible. 

## Prediction postprocessing : 

### Visualization of the errors depending threshold for belonging to thepositive class

```{r}
# plot_grid(evoSeuil(Predictions$predLda, datas$test$popularity, datas$test, "posterior"),
#           evoSeuil(Predictions$predlog$prob, datas$test$popularity, datas$test, "autres"),
#           labels=c("A", "B"), ncol = 2, nrow = 1)
```

```{r}
par(mfrow=c(2,2))
evoSeuil(Predictions$predLda, datas$test$popularity, datas$test, "posterior")
evoSeuil(Predictions$predlog$prob, datas$test$popularity, datas$test, "autres")
evoSeuil(attr(Predictions$predSvm, "probabilities")[,2],
         datas$test$popularity, datas$test, "autres")
evoSeuil(Predictions$predrf$prob[,2], datas$test$popularity, datas$test, "autres")
```

### Predictions with new threshold : 

```{r}
predldaseuil <- changeSeuil(Predictions$predLda, datas$test$popularity, datas$test, 0.2, "posterior")

predlogseuil <- changeSeuil(Predictions$predlog$prob, datas$test$popularity, datas$test, 0.2, "autres")

predrfseuil <- changeSeuil(Predictions$predrf$prob[,2], datas$test$popularity, datas$test, 0.2, "autres")

predsvmseuil <- changeSeuil(attr(Predictions$predSvm, "probabilities")[,2], datas$test$popularity, datas$test, 0.2, "autres")

listPredseuil <- list(predrfseuil, predlogseuil,
                      predldaseuil, predsvmseuil)
```

```{r}
KablesPerf2(pred = Predictions, dat = datas$test, y = "popularity", listPred = listPredseuil)
```

# Combination of the three remedies step for the best model

# Notes / Tests/ à voir pr correction / fourretout

```{r}
# Avec CARET
confusionMatrix(predLda$class, datas[["test"]]$popularity)
```

```{r}
# Avec proc 

# Pourquoi pas la même qu'au dessus ?????????????
# control and case group ????? (rapport avec réalité et prédiction / levels?)
rocLDA <- roc(datas[["test"]]$popularity, ordered(predLda$class))
plot(rocLDA)
rocLDA$auc
```

```{r}
Dat <- split_standard(spot_B, "popularity")
```

```{r}
source("functions_UC.R")
```

```{r}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "gini",
  .min.node.size = 5
)

testMod <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = tuneGrid)
```

```{r}
testMod2 <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = NULL,
                 tuneLength = 1 ) 
```

```{r}
# models2 <- function(y, data, tuneLenght = 1, tuneGridglm = NULL,
#                     costSvm = 1, kernelSv = "polynomial",
#                     weights = c(0.5,0.5)){
#   Modlda <- train(popularity~.,
#                   data = data,
#                   method = "LDA",
#                   tuneLenght = tuneLenght
#                   )
#   Modlr <- train(popularity~.,
#                  method = "glmnet",
#                  tuneLenght = tuneLenght,
#                  tuneGrid = tuneGridglm
#                  )
#   modSvm <- train(popularity~.,
#                   method = "svm",
#                   probability = TRUE,
#                   cost = costSvm,
#                   kernel = kernelSvm
#                   )
# }
```

```{r}
length(datas$train) - 1
```

weigths pour logit : need pour chaque observation!!! different de classweight

```{r}
# Améliorer la fonction evoseuil pour choisir quel taux parmi(TP, FP, FN, TN) on veux afficher
evoSeuilTPR <- function(pred, realCl, real, mod) {
  N <- sum(realCl == 0)
  P <- sum(realCl == 1)
  Error <- NULL
  FPr <- NULL
  TPr <- NULL
  for (i in 1:101) {
    c <- (i - 1) / 100
    Prediction <- rep(0, nrow(real))
    if (mod == "autres")
    {
      Prediction[pred > c] <- 1
    }
    else if (mod == "posterior")
    {
      Prediction[pred$posterior[, 2] > c] <- 1
    }
    Error[i] <- sum(Prediction != realCl) / nrow(real)
    FPr[i] <- sum((Prediction == 1) & (realCl == 0)) / N
    TPr[i] <- sum((Prediction == 1) & (realCl == 1)) / P
  }
  par(cex = 0.7)
  plot((0:100) / 100,
       Error, type = "l",
       xlim = c(0, 1), ylim = c(0, 1),
       ylab = "Taux d'erreur", xlab = "Seuil"
  )
  par(new = T)
  plot((0:100) / 100,
       FPr, type = "l", xlim = c(0, 1),
       ylim = c(0, 1), ylab = "", xlab = "",
       xaxt = "n", yaxt = "n", col = "orange"
  )
  par(new = T)
  plot((0:100) / 100,
       TPr, type = "l",
       xlim = c(0, 1), ylim = c(0, 1), ylab = "", xlab = "",
       xaxt = "n", yaxt = "n", col = "green", lty = "dashed"
  )
  legend(
    'topright',
    legend = c("Error ", "FP rate", "TP rate"),
    col = c("black", "orange", "green"), pch = 15,
    bty = "n", pt.cex = 1,
    cex = 0.8, horiz = FALSE, inset = c(0.1, 0.1)
  )
  title("Evolution des taux de TP et FP")
}
```

```{r}
evoSeuilTPR(attr(predSmote$predSvm, "probabilities")[,1],
         datasSmote$test$class, datasSmote$test, "autres")
```

This graphics shows that FP rate is always bigger than the TP rate, which confirms the ROC Curve. Results non in adequation with the confusion matrix.

Why the ROC curve is on the wrong side for SVM ??

!!!! 2h pour trouver ça espèce de gros boulet !!!!

```{r}
str(attr(Predictions$predSvm, "probabilities"))
str(attr(predSmote$predSvm, "probabilities"))
attr(predSmote$predSvm, "probabilities")[,"1"]
```

Answer : levels of attr(Predictions$predSvm, "probabilities") are not in the same order.

Note for svm : 
  - weight class parameter
  - radial or polynomial, ...
  - cost : The cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.
  
"There are many reasons why this may happen, I'd suggest you to do the following:

- Normalize your data.
- Make sure your classes are more or less balanced (have similar size). If they don't, use parameter -w to assign them different weights.
- Try different C and γ. Polynomial kernel in LIBSVM also has parameter 'coef0', as the kernel is γ⋅u′⋅v+coeff degree0"

```{r}
set.seed(1983)
svmFitW <- svm(
  popularity ~ .,
  data = datas$train,
  scale = FALSE,
  kernel = "radial",
  cost = 10,
  class.weights = c("0" = 1, "1" = 4)
)
```

```{r}
predict.svmW <- predict(svmFitW, newdata = datas$test)
mean(predict.svmW==datas$test$popularity)
confusionMatrix(predict.svmW,datas$test$popularity )
```

The gamma parameter in the RBF kernel determines the reach of a single training instance. If the value of Gamma is low, then every training instance will have a far reach. Conversely, high values of gamma mean that training instances will have a close reach. So, with a high value of gamma, the SVM decision boundary will simply be dependent on just the points that are closest to the decision boundary, effectively ignoring points that are farther away. In comparison, a low value of gamma will result in a decision boundary that will consider points that are further from it. As a result, high values of gamma typically produce highly flexed decision boundaries, and low values of gamma often results in a decision boundary that is more linear.
