---
title: "Spotify"
author: "Thibault FUCHEZ"
date: "01/06/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package

```{r}
library (tidyverse) # collection of packages for data analysis
library(caret) # Classification And REgression Training
library(pROC) # Tools for visualizing, smoothing and comparing ROC.
library(ROCR) # evaluating and visualizing classifier performance
library(lubridate) # R commands for date-times
library(tidymodels) #  collection of packages for modeling and machine learning using tidyverse principles. pre-process/train/validate
library(MASS) # Modern Applied Statistics with S" for regression and classification
library(stargazer) # Well-Formatted Regression and Summary Statistics Tables
library(randomForest) # ensemble learning method with multitude of decision tree 
library(doParallel) # paralléliser le calcul pour que ce soit plus rapide en créant un cluster de cœurs.
library(parallel) # détection du nombre de coeurs
library(ranger) # A Fast Implementation of Random Forests. (tune rf)
library(e1071) # Misc Functions of the Department of Statistics, Probability Theory Group / for svm, tune, predict, ...
library(kableExtra) # Create Awesome HTML Table
library(smotefamily) # Synthetic Minority Oversampling TEchnique
library(ROSE) # Generation of synthetic data by Randomly Over Sampling Examples
library(gdata) # Various R programming tools for data manipulation / rename object with mv("oldname, newname")
library(naivebayes) #naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem
```

```{r}
setwd("C:/Users/arkan/Desktop/Memoire-M1/applications_R")
```

```{r}
source("functions_UC.R")
```
  
# Préparation de la base de données ...

## En amont 

```{r}
#On charge la base de donnée. Les variables de types "str" sont recodées en facteurs.
spotify <-
  read.csv(
    "spotify_songs.csv",
    header = TRUE,
    sep = ",",
    stringsAsFactors = T
  )

# Etant données qu'il n'y a que 15 valeurs manquantes, on décide de supprimer les individus contenant ces valeurs.
sum(is.na(spotify))
spotify = na.omit(spotify)

# Lignes dupliquées :
spot1 <- spotify[!duplicated(spotify$track_id), ]
# spot2 <- spotify
# spot2 %>% distinct(track_id)
# spot3 <- spotify[duplicated(spot2$track_id),]
# spot4 <- spotify[!duplicated(spotify$track_name),]

#Changement du nom des levels:
spot1$key <- recode_factor(
  spot1$key,
  "0" = "C",
  "1" = "Db",
  "2" = "D",
  "3" = "Eb",
  "4" = "E",
  "5" = "F",
  "6" = "Gb",
  "7" = "G",
  "8" = "Ab",
  "9" = "A",
  "10" = "Bb",
  "11" = "B"
)

spot1$mode <-
  recode_factor(spot1$mode, "0" = "mineur", "1" = "majeur")

# recodage de la date / regroupement par tranches :
spot1$track_album_release_date <-
  as.Date(spot1$track_album_release_date , format = "%Y")
spot1 <- spot1 %>% mutate(year = year(track_album_release_date))
spot1$years <-
  cut(
    spot1$year,
    breaks = c(1956, 2000, 2010, 2015, 2018, 2020),
    diag.lab = 0,
    labels = c(
      "1956-2000",
      "2001-2010",
      "2011-2015",
      "2016-2018",
      "2019-2020"
    )
  )

# Suppression de instrumentalness mal codé :
spot1 <- spot1[, -c(7, 19)]

# Recodage de track-popularity / regroupement basse/autres :
popularity <- cut(spot1$track_popularity, c(-1, 12.5, 100))
spot_B = cbind(popularity, spot1)
spot_B <- spot_B[, -5]
# spot_B$popularity <- recode_factor(spot_B$popularity,
#                                      "(12.5,100]" = "Autre",
#                                      "(-1,12.5]" = "basse")
spot_B$popularity <- recode_factor(spot_B$popularity,
                                   "(12.5,100]" = 0,
                                   "(-1,12.5]" = 1)


# On enlève les variables non pertinentes pour la classification :
spot_B <- spot_B[, -c(2, 3, 4, 5, 6, 7, 8, 10)]

# On efface les bases inutiles
rm(spot1, spotify)
```

```{r}
plot(spot_B$popularity)
```

## Partitionnement : 

The training and test sets were created using stratiﬁed random sampling. First, split the training set off. Then create the evaluation and test sets. Hence we normalize/standardize the datas. 

```{r}
#posssibilité de changer les paramètres prop1, prop2, seed1, seed2
datas <- split_standard2(spot_B, "popularity")

# verif
sum(nrow(datas$train),nrow(datas$test),nrow(datas$eval))==nrow(spot_B)
```

```{r}
# Version avec moins de paramètre
# datas <- split_standard(spot_B, "popularity")
```

# First Models : 

```{r}
Models <- models(y = "popularity", data = datas$train)
```

```{r}
Predictions <- predictions(models = Models, datas = datas)
```

```{r}
listPred1 <- list(Predictions$predrf, Predictions$predlog, Predictions$predSvm, Predictions$predLda$class)
lapply(listPred1, perf.measure, realCl = datas$test$popularity,
       real = datas$test)
```

We cans emphasize the low efficiency of accuracy when we faces imblanced sample. Hence, depsite a very good accuracy, we can see than false alarm and detection power are not good. 

# Remedies 

## Datas prepocessing : 

### SMOTE 

Synthetic Minority Oversampling Technique

need numeric data only : 

```{r}
datas_num<- spot_B[, -c(2,5,7,15)]
```

```{r}
smote <- SMOTE(datas_num[,-1],datas_num[,1])
```

```{r}
Smotedata <- smote$data
Smotedata$class<-as.factor(Smotedata$class)
str(Smotedata)
plot(Smotedata$class)
Smotedata$popularity <- Smotedata$class
Smotedata <- Smotedata[,-11]
```

```{r}
datasSmote <- split_standard2(Smotedata, "popularity")
```

```{r}
ModelsSmote <- models(y = "popularity", data = datasSmote$train)
```

```{r}
predSmote <- predictions(models = ModelsSmote, datas = datasSmote)
```

```{r}
listpredSmote <- list(predSmote$predrf, predSmote$predlog, predSmote$predSvm, predSmote$predLda$class)
lapply(listpredSmote, perf.measure, realCl = datasSmote$test$popularity,
       real = datasSmote$test)
```

### ROSE

Random Over-Sampling Examples

```{r}
rose <- ROSE(popularity~. , spot_B)
RoseData <- rose$data
plot(RoseData$popularity)
```

```{r}
datasRose<- split_standard2(RoseData, "popularity")
```

```{r}
ModelsRose <- models(y = "popularity", data = datasRose$train)
```

```{r}
predRose <- predictions(models = ModelsRose, datas = datasRose)
```

```{r}
listpredRose <- list(predRose$predrf, predRose$predlog, predRose$predSvm, predRose$predLda$class)
lapply(listpredRose, perf.measure, realCl = datasRose$test$popularity,
       real = datasRose$test)
```

## Learning method tuning : 

## Prediction postprocessing : 

* alternate cutoff LDA

```{r}
evoSeuil(Predictions$predLda, datas$test$popularity, datas$test, "autre")
```

```{r}
predldaseuil <- changeSeuil(Predictions$predLda, datas$test$popularity, datas$test, 0.2, "autre")
predldaseuil
```

```{r}
perf.measure(predldaseuil, datas$test$popularity, datas$test)
```

* alternate cutoff rf

pred


