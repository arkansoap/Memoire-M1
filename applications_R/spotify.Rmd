---
title: "Spotify"
author: "Thibault FUCHEZ"
date: "01/06/2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Package

```{r}
library (tidyverse) # collection of packages for data analysis
library(caret) # Classification And REgression Training
library(pROC) # Tools for visualizing, smoothing and comparing ROC.
library(ROCR) # evaluating and visualizing classifier performance
library(lubridate) # R commands for date-times
library(tidymodels) #  collection of packages for modeling and machine learning using tidyverse principles. pre-process/train/validate
library(MASS) # Modern Applied Statistics with S" for regression and classification
library(stargazer) # Well-Formatted Regression and Summary Statistics Tables
library(randomForest) # ensemble learning method with multitude of decision tree 
library(doParallel) # paralléliser le calcul pour que ce soit plus rapide en créant un cluster de cœurs.
library(parallel) # détection du nombre de coeurs
library(ranger) # A Fast Implementation of Random Forests. (tune rf)
library(e1071) # Misc Functions of the Department of Statistics, Probability Theory Group / for svm, tune, predict, ...
library(kableExtra) # Create Awesome HTML Table
library(smotefamily) # Synthetic Minority Oversampling TEchnique
library(ROSE) # Generation of synthetic data by Randomly Over Sampling Examples
library(gdata) # Various R programming tools for data manipulation / rename object with mv("oldname, newname")
library(naivebayes) #naive Bayes classifiers are a family of simple "probabilistic classifiers" based on applying Bayes' theorem
library(glmnet) # Lasso and Elastic-Net Regularized Generalized Linear Models
```

```{r}
setwd("C:/Users/arkan/Desktop/Memoire-M1/applications_R")
```

```{r}
source("functions_UC.R")
```
  
# Preparation of the data base

## restructuring 

```{r}
#On charge la base de donnée. Les variables de types "str" sont recodées en facteurs.
spotify <-
  read.csv(
    "spotify_songs.csv",
    header = TRUE,
    sep = ",",
    stringsAsFactors = T
  )

# Etant données qu'il n'y a que 15 valeurs manquantes, on décide de supprimer les individus contenant ces valeurs.
sum(is.na(spotify))
spotify = na.omit(spotify)

# Lignes dupliquées :
spot1 <- spotify[!duplicated(spotify$track_id), ]
# spot2 <- spotify
# spot2 %>% distinct(track_id)
# spot3 <- spotify[duplicated(spot2$track_id),]
# spot4 <- spotify[!duplicated(spotify$track_name),]

#Changement du nom des levels:
spot1$key <- recode_factor(
  spot1$key,
  "0" = "C",
  "1" = "Db",
  "2" = "D",
  "3" = "Eb",
  "4" = "E",
  "5" = "F",
  "6" = "Gb",
  "7" = "G",
  "8" = "Ab",
  "9" = "A",
  "10" = "Bb",
  "11" = "B"
)

spot1$mode <-
  recode_factor(spot1$mode, "0" = "mineur", "1" = "majeur")

# recodage de la date / regroupement par tranches :
spot1$track_album_release_date <-
  as.Date(spot1$track_album_release_date , format = "%Y")
spot1 <- spot1 %>% mutate(year = year(track_album_release_date))
spot1$years <-
  cut(
    spot1$year,
    breaks = c(1956, 2000, 2010, 2015, 2018, 2020),
    diag.lab = 0,
    labels = c(
      "1956-2000",
      "2001-2010",
      "2011-2015",
      "2016-2018",
      "2019-2020"
    )
  )

# Suppression de instrumentalness mal codé :
spot1 <- spot1[, -c(7, 19)]

# Recodage de track-popularity / regroupement basse/autres :
popularity <- cut(spot1$track_popularity, c(-1, 12.5, 100))
spot_B = cbind(popularity, spot1)
spot_B <- spot_B[, -5]
# spot_B$popularity <- recode_factor(spot_B$popularity,
#                                      "(12.5,100]" = "Autre",
#                                      "(-1,12.5]" = "basse")
spot_B$popularity <- recode_factor(spot_B$popularity,
                                   "(12.5,100]" = 0,
                                   "(-1,12.5]" = 1)


# On enlève les variables non pertinentes pour la classification :
spot_B <- spot_B[, -c(2, 3, 4, 5, 6, 7, 8, 10,23)]

# On efface les bases inutiles
rm(spot1, spotify)
```

```{r}
plot(spot_B$popularity)
```

We can observe a high imbalanced repartition on the "popularity" variable.

## Partitioning : 

Training and test sets are created with random sampling. First, we split the training set off. Then whe create the evaluation and test sets. Hence we create two groups of datas, one which is standardize and the other which is not. You can see the functions used in the functions_UCR.R file.

```{r}
#posssibilité de changer les paramètres prop1, prop2, seed1, seed2
datas <- split_standard(spot_B, "popularity", mod = "standard")
datasNS <- split_standard(spot_B, "popularity", mod = "nonstandard")
# verif
sum(nrow(datas$train),nrow(datas$test),nrow(datas$eval))==nrow(spot_B)
```

# First Models : 

To introduce our problematics, we fit four kind of models withs basics parameters. We choose LDA, RF, LR and SVM (a brief overview of each model is available in the summary document).

The priors of the training set are :

```{r}
priors(dat = datas$train, "popularity")
```

```{r}
Models <- models(y = "popularity", data = datas$train,
                 prior = priors(dat = datas$train, "popularity"))
# ModelsNS <- models(y = "popularity", data = datasNS$train)
```

With current parameter, The SVM model fails to find an hyperplane with non normalize datas. This errors occurs when whe faces imbalance data. 

```{r}
Predictions <- predictions(models = Models, datas = datas)
```

```{r}
kab1 <- KablesPerf(pred = Predictions, dat = datas$test, y = "popularity")

kab1[[1]]
kab1[[2]]
```

```{r}
AllRoc(Predictions, datas$test$popularity)
```

We can emphasize the low efficiency of accuracy when we faces imblanced sample. Hence, depsite a very good accuracy, we can see than false alarm and detection power are very bad. 
Note that RF has pretty good results in comparison of the other models.
We clearly observe that our models are inefficient. Hence, because of the small size of the positive class, the models tends to classify almost all individuals in the minority class (not enough datas to learn about positive class structure). Linked to the known fact that there is a large majority of individuals in the minority level, We can understand the very good score of accuracy.

# Remedies 

A lot of research have been made concerning this problem. Our goal is not to make an exhaustive review of all the technics to remedies this issue. 
In this study, we will focus on methods than we can repoduce with our level of competence. It appears to us that it is interesting to separate the choosen methods in three levels :
- First, some remedies we can use before launching the machine learning algorithm (Preprocessing).
- Secundly, some remedies we can use during the computation of a fitted models by the machine (learning method tuning).
- At last, som remedies that can be used after the machine learning algorithm (postprocessing).

## Datas prepocessing : 


### CARET up and down sample : 

"downSample will randomly sample a data set so that all classes have the same frequency as the minority class. upSample samples with replacement to make the class distributions equal"

#### Random downsample

```{r}
DownSampleRandom <- downSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(DownSampleRandom$Class)
```

```{r}
datasRDS <- split_standard(DownSampleRandom, "Class", mod = "standard")
```

```{r}
ModelsRDS <- models(y = "Class", data = datasRDS$train,
                    prior = priors(datasRDS$train, "Class"))
```

```{r}
PredictionsRDS <- predictions(models = ModelsRDS, datas = datasRDS)
```

```{r}
KablesPerf(pred = PredictionsRDS, dat = datasRDS$test, y = "Class")
```

```{r}
AllRoc(predic = PredictionsRDS, dataCl = datasRDS$test$Class)
```

A first try with using Random down-resampling. Here we randomly reduce the large class (negative class). The major default of this method is that we significantly reduces the data base which one the algorithm learn. We easily understand than the less cases the machine has, the worst reliable the predictions are.

RF is still the best model, following by SVM. 

A major issue is the False alarm rate (FP rate). 

To get a better compromise beetween sensitivity(TPrate) and specificity(TNrate), we need to pay it in glabal accuracy. It cost in accuracy (and False alarm) to gain a better detection power. 

However, Results are globally better than with the models without resampling.

#### Random upsample

```{r}
UpSampleRandom <- upSample(spot_B[,-1], spot_B[,1])
```

```{r}
plot(UpSampleRandom$Class)
```

```{r}
datasRUS <- split_standard(UpSampleRandom, "Class", mod = "standard")
```

```{r}
ModelsRUS <- models(y = "Class", data = datasRUS$train,
                    prior = priors(datasRUS$train, "Class"))
```

```{r}
PredictionsRUS <- predictions(models = ModelsRUS, datas = datasRUS)
```

```{r}
KablesPerf(pred = PredictionsRUS, dat = datasRUS$test, y = "Class")
```

```{r}
AllRoc(predic = PredictionsRUS, dataCl = datasRUS$test$Class)
```

Excellent results for random forest !! How can we explain it ? Maybe because we standardize the data base?

### SMOTE 

Synthetic Minority Oversampling Technique

need numeric data only : 

```{r}
datas_num<- spot_B[, -c(2,5,7)]
```

```{r}
Smote <- SMOTE(datas_num[,-1],datas_num[,1])
```

```{r}
Smotedata <- Smote$data
Smotedata$class<-as.factor(Smotedata$class)
```

```{r}
plot(Smotedata$class)
```

```{r}
datasSmote <- split_standard(Smotedata, "class", mod = "standard")
```

```{r}
ModelsSmote <- models(y = "class", data = datasSmote$train,
                      prior = priors(datasSmote$train, "class"))
```

```{r}
predSmote <- predictions(models = ModelsSmote, datas = datasSmote)
```

```{r}
KablesPerf(pred = predSmote, dat = datasSmote$test, y = "class")
```

```{r}
AllRoc(predic = predSmote, dataCl = datasSmote$test$class)
```

Why the ROC curve is on the wrong side for SVM ??

We are reaching the same conclusion than upsampling. The actual results are less "extreme". Possibly due to the fact that RUS create too much individuals.  Develop ... 

### BLSmote

### ADASYN

### ROSE

Random Over-Sampling Examples

note : look more about rose.eval function.

```{r}
rose <- ROSE(popularity~. , spot_B)
RoseData <- rose$data
```

```{r}
plot(RoseData$popularity)
```

```{r}
datasRose<- split_standard(RoseData, "popularity", mod = "standard")
```

```{r}
ModelsRose <- models(y = "popularity", data = datasRose$train,
                     prior = priors(datasRose$train, "popularity"))
```

```{r}
predRose <- predictions(models = ModelsRose, datas = datasRose)
```

```{r}
KablesPerf(predRose, datasRose$test, "popularity")
```

```{r}
AllRoc(predRose, dataCl = datasRose$test$popularity)
```

## Learning method tuning : 

Note for svm : 
  - weight class parameter
  - radial or polynomial, ...
  - cost : The cost of constraints violation (default: 1)—it is the ‘C’-constant of the regularization term in the Lagrange formulation.
  
"There are many reasons why this may happen, I'd suggest you to do the following:

- Normalize your data.
- Make sure your classes are more or less balanced (have similar size). If they don't, use parameter -w to assign them different weights.
- Try different C and γ. Polynomial kernel in LIBSVM also has parameter 'coef0', as the kernel is γ⋅u′⋅v+coeff degree0"

### Tune parameter with no direct link with imbalanced data : 

*RF

```{r}
# enregistrement d'un cluster avec le package doParallel
library(doParallel)
registerDoParallel(cores = 6)

rangerGrid <- expand.grid(
  mtry = c(1, 2, 4, 8, 14),
  min.node.size = c(5, 10, 50, 100),
  splitrule = "gini"
)
ctrlCv <-
  trainControl(method = "repeatedcv",
               repeats = 3,
               number = 5)

system.time(
  ranger.rf <- train(
    popularity ~ .,
    data = datas$train,
    method = "ranger",
    trControl = ctrlCv,
    tuneGrid = rangerGrid
  )
)

# fermeture du cluster
stopImplicitCluster()

# pour CV, paramètre number et reapets ?
# na.action not necessary because all na were ommitted
```

```{r}
ranger.rf
ranger.rf$bestTune
ranger.rf$finalModel
```

* SVM

```{r}
registerDoParallel(cores = 6)

system.time(
  tuneSvm <- tune(
    svm,
    popularity ~ .,
    data = datas$test,
    ranges = list(gamma = c(0.1, 1, 10), cost = 10 ^ (1:3)),
    tunecontrol = tune.control(
      nrepeat = 10,
      sampling = "cross",
      cross = 10
    )
  )
)


stopImplicitCluster()
```

```{r}
tuneSvm
```

### Tune Class weight 

* SVM

```{r}
set.seed(1983)
svmFitW <- svm(
  popularity ~ .,
  data = datas$train,
  scale = FALSE,
  kernel = "radial",
  cost = 10,
  class.weights = c("0" = 1, "1" = 4)
)
```

```{r}
predict.svmW <- predict(svmFitW, newdata = datas$test)
mean(predict.svmW==datas$test$popularity)
confusionMatrix(predict.svmW,datas$test$popularity )
```

## Prediction postprocessing : 

```{r}
predbasic <- predictions(models = Models, datas = datas)
```

* alternate cutoff LDA

```{r}
evoSeuil(Predictions$predLda, datas$test$popularity, datas$test, "autre")
```

```{r}
predldaseuil <- changeSeuil(predbasic$predLda, datas$test$popularity, datas$test, 0.2, "autre")
```

```{r}
perf.measure(predldaseuil, datas$test$popularity, datas$test)
```

* alternate cut off logit

```{r}
evoSeuil(predbasic$predlog$prob, datas$test$popularity, datas$test, "logit")
```

```{r}
predlogitseuil <- changeSeuil(predbasic$predlog$prob, datas$test$popularity, datas$test, 0.2, "logit")
```

```{r}
perf.measure(predlogitseuil, datas$test$popularity, datas$test)
```

* alternate cutoff rf

```{r}
evoSeuil(pred = predbasic$predrf$prob[,2], datas$test$popularity, datas$test, "logit")
```

```{r}
predrfseuil <- changeSeuil(predbasic$predrf$prob[,2], datas$test$popularity, datas$test, 0.2, "logit")
```

```{r}
perf.measure(predrfseuil, datas$test$popularity, datas$test)
```

*alternate cutoff svm

Changer fonction predictions pour sortir les prob de svm si possible 

```{r}
svmtest <- svm(
    popularity~.,
    data = datas$train,
    scale = FALSE,
    kernel = "polynomial",
    cost = 5,
    probability = TRUE
)
```

```{r}
testprobsvm <- predict(Models$ModSvm, newdata = datas$test, probability = TRUE)
```

```{r}
AA <- attr(testprobsvm, "probabilities")[,2]
```

```{r}
evoSeuil(AA, datas$test$popularity, datas$test, "logit")
```

```{r}
AA2 <- changeSeuil(pred = AA, datas$test$popularity, datas$test, 0.19, "logit" )
```

```{r}
perf.measure2(AA2, datas$test$popularity, datas$test)
```

```{r}
perf.measure(AA2, datas$test, "popularity")
```


# Combination of the three remedies step for the best model

# Notes / Tests/ à voir pr correction

```{r}
# Avec CARET
confusionMatrix(predLda$class, datas[["test"]]$popularity)
```

```{r}
# Avec proc 

# Pourquoi pas la même qu'au dessus ?????????????
# control and case group ????? (rapport avec réalité et prédiction / levels?)
rocLDA <- roc(datas[["test"]]$popularity, ordered(predLda$class))
plot(rocLDA)
rocLDA$auc
```

```{r}
Dat <- split_standard(spot_B, "popularity")
```

```{r}
source("functions_UC.R")
```

```{r}
tuneGrid <- data.frame(
  .mtry = c(2, 3, 7),
  .splitrule = "gini",
  .min.node.size = 5
)

testMod <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = tuneGrid)
```

```{r}
testMod2 <- train(popularity~.,
                 data = datas$train,
                 method = "ranger",
                 tuneGrid = NULL,
                 tuneLength = 1 ) 
```

```{r}
# models2 <- function(y, data, tuneLenght = 1, tuneGridglm = NULL,
#                     costSvm = 1, kernelSv = "polynomial",
#                     weights = c(0.5,0.5)){
#   Modlda <- train(popularity~.,
#                   data = data,
#                   method = "LDA",
#                   tuneLenght = tuneLenght
#                   )
#   Modlr <- train(popularity~.,
#                  method = "glmnet",
#                  tuneLenght = tuneLenght,
#                  tuneGrid = tuneGridglm
#                  )
#   modSvm <- train(popularity~.,
#                   method = "svm",
#                   probability = TRUE,
#                   cost = costSvm,
#                   kernel = kernelSvm
#                   )
# }
```

```{r}
length(datas$train) - 1
```

weigths pour logit : need pour chaque observation!!! different de classweight

